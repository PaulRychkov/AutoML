# Health Insurance Cross Sell Prediction Research Notebook

## –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞

–≠—Ç–æ—Ç –±–ª–æ–∫–Ω–æ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ú—ã –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∞–∫—Ü–∏—é –∫–ª–∏–µ–Ω—Ç–æ–≤ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –º–µ–¥ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è. –ó–∞–¥–∞—á–∞ –≤–∑—è—Ç–∞ –∏–∑ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è Kaggle Playground Series Season 4 Episode 7.

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:

1. **EDA (Exploratory Data Analysis)**: –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
2. **LightAutoML Baseline**: –±–∞–∑–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LightAutoML, –¥–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
3. **Custom Solution**: —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LightAutoML
   * –ü—Ä–æ—Å—Ç—ã–µ pipeline –ø–æ–¥—Ö–æ–¥—ã
   * –£–ª—É—á—à–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã —Å –æ—á–∏—Å—Ç–∫–æ–π –¥–∞–Ω–Ω—ã—Ö
   * CatBoost —Ä–µ—à–µ–Ω–∏—è: –ø—Ä–æ—Å—Ç–æ–π, –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π, —Å Optuna
4. **–í—ã–≤–æ–¥—ã –∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è**: –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ

### –ú–µ—Ç—Ä–∏–∫–∞: ROC-AUC

## 1. –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è


```python
import os
import warnings
import time
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import roc_auc_score, roc_curve, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
import lightgbm as lgb
from catboost import CatBoostClassifier, Pool
import optuna
import joblib
from scipy import stats

from lightautoml.automl.presets.tabular_presets import TabularAutoML
from lightautoml.tasks import Task

warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (12, 6)

RANDOM_STATE = 42
N_THREADS = 16
N_FOLDS = 5
TEST_SIZE = 0.2
TIMEOUT = 1800

DATA_DIR = Path('../playground-series-s4e7')
if not DATA_DIR.exists():
    DATA_DIR = Path('playground-series-s4e7')
MODELS_DIR = Path('src/models')
MODELS_DIR.mkdir(exist_ok=True)

PARAMS_DIR = Path('src/params')
PARAMS_DIR.mkdir(exist_ok=True)

SUBMIT_DIR = Path('src/submission')
SUBMIT_DIR.mkdir(exist_ok=True)

OTHER_DIR = Path('src/other')
OTHER_DIR.mkdir(exist_ok=True)

print("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã")
print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö: {DATA_DIR.absolute()}")
print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–µ–π: {MODELS_DIR.absolute()}")
print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {PARAMS_DIR.absolute()}")
print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è submission: {SUBMIT_DIR.absolute()}")
print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥—Ä—É–≥–∏—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤: {OTHER_DIR.absolute()}")
```

## 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö


```python
train_data = pd.read_csv(DATA_DIR / 'train.csv')
test_data = pd.read_csv(DATA_DIR / 'test.csv')
sample_submission = pd.read_csv(DATA_DIR / 'sample_submission.csv')

print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {train_data.shape}")
print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {test_data.shape}")
print(f"\n–ö–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö:")
print(train_data.columns.tolist())
print(f"\n–ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏:")
train_data.head()
```

    –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: (11504798, 12)
    –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: (7669866, 11)
    
    –ö–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö:
    ['id', 'Gender', 'Age', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage', 'Response']
    
    –ü–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏:
    




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>Gender</th>
      <th>Age</th>
      <th>Driving_License</th>
      <th>Region_Code</th>
      <th>Previously_Insured</th>
      <th>Vehicle_Age</th>
      <th>Vehicle_Damage</th>
      <th>Annual_Premium</th>
      <th>Policy_Sales_Channel</th>
      <th>Vintage</th>
      <th>Response</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>Male</td>
      <td>21</td>
      <td>1</td>
      <td>35.0</td>
      <td>0</td>
      <td>1-2 Year</td>
      <td>Yes</td>
      <td>65101.0</td>
      <td>124.0</td>
      <td>187</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>Male</td>
      <td>43</td>
      <td>1</td>
      <td>28.0</td>
      <td>0</td>
      <td>&gt; 2 Years</td>
      <td>Yes</td>
      <td>58911.0</td>
      <td>26.0</td>
      <td>288</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>Female</td>
      <td>25</td>
      <td>1</td>
      <td>14.0</td>
      <td>1</td>
      <td>&lt; 1 Year</td>
      <td>No</td>
      <td>38043.0</td>
      <td>152.0</td>
      <td>254</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>Female</td>
      <td>35</td>
      <td>1</td>
      <td>1.0</td>
      <td>0</td>
      <td>1-2 Year</td>
      <td>Yes</td>
      <td>2630.0</td>
      <td>156.0</td>
      <td>76</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>Female</td>
      <td>36</td>
      <td>1</td>
      <td>15.0</td>
      <td>1</td>
      <td>1-2 Year</td>
      <td>No</td>
      <td>31951.0</td>
      <td>152.0</td>
      <td>294</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
print("–¥–∞–Ω–Ω—ã–µ:")
print(train_data.info())
print("\n" + "="*50)
print("\n–û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
print(train_data.describe())
print("\n" + "="*50)
print("\n–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:")
missing = train_data.isnull().sum()
print(missing[missing > 0] if missing.sum() > 0 else "–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
```

    –¥–∞–Ω–Ω—ã–µ:
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 11504798 entries, 0 to 11504797
    Data columns (total 12 columns):
     #   Column                Dtype  
    ---  ------                -----  
     0   id                    int64  
     1   Gender                object 
     2   Age                   int64  
     3   Driving_License       int64  
     4   Region_Code           float64
     5   Previously_Insured    int64  
     6   Vehicle_Age           object 
     7   Vehicle_Damage        object 
     8   Annual_Premium        float64
     9   Policy_Sales_Channel  float64
     10  Vintage               int64  
     11  Response              int64  
    dtypes: float64(3), int64(6), object(3)
    memory usage: 1.0+ GB
    None
    
    ==================================================
    
    –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
                     id           Age  Driving_License   Region_Code  \
    count  1.150480e+07  1.150480e+07     1.150480e+07  1.150480e+07   
    mean   5.752398e+06  3.838356e+01     9.980220e-01  2.641869e+01   
    std    3.321149e+06  1.499346e+01     4.443120e-02  1.299159e+01   
    min    0.000000e+00  2.000000e+01     0.000000e+00  0.000000e+00   
    25%    2.876199e+06  2.400000e+01     1.000000e+00  1.500000e+01   
    50%    5.752398e+06  3.600000e+01     1.000000e+00  2.800000e+01   
    75%    8.628598e+06  4.900000e+01     1.000000e+00  3.500000e+01   
    max    1.150480e+07  8.500000e+01     1.000000e+00  5.200000e+01   
    
           Previously_Insured  Annual_Premium  Policy_Sales_Channel       Vintage  \
    count        1.150480e+07    1.150480e+07          1.150480e+07  1.150480e+07   
    mean         4.629966e-01    3.046137e+04          1.124254e+02  1.638977e+02   
    std          4.986289e-01    1.645475e+04          5.403571e+01  7.997953e+01   
    min          0.000000e+00    2.630000e+03          1.000000e+00  1.000000e+01   
    25%          0.000000e+00    2.527700e+04          2.900000e+01  9.900000e+01   
    50%          0.000000e+00    3.182400e+04          1.510000e+02  1.660000e+02   
    75%          1.000000e+00    3.945100e+04          1.520000e+02  2.320000e+02   
    max          1.000000e+00    5.401650e+05          1.630000e+02  2.990000e+02   
    
               Response  
    count  1.150480e+07  
    mean   1.229973e-01  
    std    3.284341e-01  
    min    0.000000e+00  
    25%    0.000000e+00  
    50%    0.000000e+00  
    75%    0.000000e+00  
    max    1.000000e+00  
    
    ==================================================
    
    –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:
    –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ
    

## 3. Exploratory Data Analysis (EDA)

–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –º—ã –∏–∑—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ. –°–º–æ—Ç—Ä–∏–º –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π. –ò—â–µ–º –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏.

### 3.1. –ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π Response. –í –¥–∞–Ω–Ω—ã—Ö –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —Å–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: –∫–ª–∞—Å—Å 0 —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 87.70% (10,089,739 –Ω–∞–±–ª—é–¥–µ–Ω–∏–π), –∞ –∫–ª–∞—Å—Å 1 —Ç–æ–ª—å–∫–æ 12.30% (1,415,059 –Ω–∞–±–ª—é–¥–µ–Ω–∏–π). –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ 7.13:1. –¢–∞–∫–æ–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –Ω—É–∂–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤ –∏–ª–∏ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏.


```python
target_col = 'Response'
target_counts = train_data[target_col].value_counts()
target_props = train_data[target_col].value_counts(normalize=True)

print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:")
print(f"–ö–ª–∞—Å—Å 0: {target_counts[0]} ({target_props[0]*100:.2f}%)")
print(f"–ö–ª–∞—Å—Å 1: {target_counts[1]} ({target_props[1]*100:.2f}%)")
print(f"\n–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π: {len(train_data)}")
print(f"–î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {target_counts[0] / target_counts[1]:.2f}:1")

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].bar(target_counts.index, target_counts.values, color=['skyblue', 'salmon'])
axes[0].set_xlabel('Response')
axes[0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
axes[0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)')
axes[0].set_xticks([0, 1])
for i, v in enumerate(target_counts.values):
    axes[0].text(i, v, str(v), ha='center', va='bottom', fontsize=12)

axes[1].pie(target_counts.values, labels=['No Response (0)', 'Response (1)'], 
            autopct='%1.2f%%', colors=['skyblue', 'salmon'], startangle=90)
axes[1].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–ø—Ä–æ—Ü–µ–Ω—Ç—ã)')

plt.tight_layout()
plt.savefig(OTHER_DIR / 'target_distribution.png', dpi=150, bbox_inches='tight')
plt.show()
```

    –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:
    –ö–ª–∞—Å—Å 0: 10089739 (87.70%)
    –ö–ª–∞—Å—Å 1: 1415059 (12.30%)
    
    –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π: 11504798
    –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: 7.13:1
    


    
![png](readme_files/readme_8_1.png)
    


### 3.2. –¢–∏–ø–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –∏—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è

–î–µ–ª–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —á–∏—Å–ª–µ–Ω–Ω—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ. –í –¥–∞–Ω–Ω—ã—Ö 7 —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Age, Driving_License, Region_Code, Previously_Insured, Annual_Premium, Policy_Sales_Channel, Vintage) –∏ 3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞ (Gender, Vehicle_Age, Vehicle_Damage). –í–æ–∑—Ä–∞—Å—Ç –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤–∞—Ä—å–∏—Ä—É–µ—Ç—Å—è –æ—Ç 20 –¥–æ 85 –ª–µ—Ç —Å–æ —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º 38.4 –≥–æ–¥–∞.


```python
numeric_cols = train_data.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = train_data.select_dtypes(include=['object']).columns.tolist()

if 'id' in numeric_cols:
    numeric_cols.remove('id')
if target_col in numeric_cols:
    numeric_cols.remove(target_col)

print("–ß–∏—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:", numeric_cols)
print(f"\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(numeric_cols)}")
print("\n–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:", categorical_cols)
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(categorical_cols)}")


```

    –ß–∏—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ['Age', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']
    
    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: 7
    
    –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ['Gender', 'Vehicle_Age', 'Vehicle_Damage']
    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: 3
    


```python
print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º:")
print(train_data[numeric_cols].describe())
```

    –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º:
                    Age  Driving_License   Region_Code  Previously_Insured  \
    count  1.150480e+07     1.150480e+07  1.150480e+07        1.150480e+07   
    mean   3.838356e+01     9.980220e-01  2.641869e+01        4.629966e-01   
    std    1.499346e+01     4.443120e-02  1.299159e+01        4.986289e-01   
    min    2.000000e+01     0.000000e+00  0.000000e+00        0.000000e+00   
    25%    2.400000e+01     1.000000e+00  1.500000e+01        0.000000e+00   
    50%    3.600000e+01     1.000000e+00  2.800000e+01        0.000000e+00   
    75%    4.900000e+01     1.000000e+00  3.500000e+01        1.000000e+00   
    max    8.500000e+01     1.000000e+00  5.200000e+01        1.000000e+00   
    
           Annual_Premium  Policy_Sales_Channel       Vintage  
    count    1.150480e+07          1.150480e+07  1.150480e+07  
    mean     3.046137e+04          1.124254e+02  1.638977e+02  
    std      1.645475e+04          5.403571e+01  7.997953e+01  
    min      2.630000e+03          1.000000e+00  1.000000e+01  
    25%      2.527700e+04          2.900000e+01  9.900000e+01  
    50%      3.182400e+04          1.510000e+02  1.660000e+02  
    75%      3.945100e+04          1.520000e+02  2.320000e+02  
    max      5.401650e+05          1.630000e+02  2.990000e+02  
    


```python
n_numeric = len(numeric_cols)
n_cols = 3
n_rows = (n_numeric + n_cols - 1) // n_cols

fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5*n_rows))
axes = axes.flatten()

for i, col in enumerate(numeric_cols):
    axes[i].hist(train_data[col].dropna(), bins=50, edgecolor='black', alpha=0.7)
    axes[i].set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    axes[i].grid(True, alpha=0.3)

for i in range(n_numeric, len(axes)):
    axes[i].axis('off')

plt.tight_layout()
plt.savefig(OTHER_DIR / 'numeric_distributions.png', dpi=150, bbox_inches='tight')
plt.show()
```


    
![png](readme_files/readme_12_0.png)
    


### 3.3. –í—ã—è–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

–ò—â–µ–º –≤—ã–±—Ä–æ—Å—ã –≤ —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö. –≠—Ç–æ –Ω—É–∂–Ω–æ —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–µ—à–∏—Ç—å –Ω—É–∂–Ω–æ –ª–∏ –∏—Ö –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª –Ω–∞–ª–∏—á–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –ø—Ä–∏–∑–Ω–∞–∫–µ Annual_Premium: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ 2,377,273 –≤—ã–±—Ä–æ—Å–∞ (20.66% –æ—Ç –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö). –í –ø—Ä–∏–∑–Ω–∞–∫–µ Driving_License –Ω–∞–π–¥–µ–Ω–æ 22,757 –≤—ã–±—Ä–æ—Å–æ–≤ (0.20%). –û—Å—Ç–∞–ª—å–Ω—ã–µ —á–∏—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –∑–Ω–∞—á–∏–º—ã—Ö –≤—ã–±—Ä–æ—Å–æ–≤. –í—ã–±—Ä–æ—Å—ã –≤ Annual_Premium –º–æ–≥—É—Ç –≤–ª–∏—è—Ç—å –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –∏ —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.


```python
def detect_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

print("–ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤ (IQR –º–µ—Ç–æ–¥):")
print("="*60)
outliers_summary = {}

for col in numeric_cols:
    outliers, lower, upper = detect_outliers_iqr(train_data, col)
    n_outliers = len(outliers)
    pct_outliers = (n_outliers / len(train_data)) * 100
    outliers_summary[col] = {
        'count': n_outliers,
        'percentage': pct_outliers,
        'lower_bound': lower,
        'upper_bound': upper
    }
    print(f"\n{col}:")
    print(f"–í—ã–±—Ä–æ—Å–æ–≤: {n_outliers} ({pct_outliers:.2f}%)")
    print(f"–ì—Ä–∞–Ω–∏—Ü—ã: [{lower:.2f}, {upper:.2f}]")

n_numeric = len(numeric_cols)
n_cols = 2
n_rows = (n_numeric + n_cols - 1) // n_cols

fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 5*n_rows))
axes = axes.flatten()

for i, col in enumerate(numeric_cols):
    axes[i].boxplot(train_data[col].dropna(), vert=True)
    axes[i].set_title(f'Boxplot –¥–ª—è {col}')
    axes[i].set_ylabel(col)
    axes[i].grid(True, alpha=0.3)

for i in range(n_numeric, len(axes)):
    axes[i].axis('off')

plt.tight_layout()
plt.savefig(OTHER_DIR / 'outliers_boxplots.png', dpi=150, bbox_inches='tight')
plt.show()
```

    –ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤ (IQR –º–µ—Ç–æ–¥):
    ============================================================
    
    Age:
    –í—ã–±—Ä–æ—Å–æ–≤: 0 (0.00%)
    –ì—Ä–∞–Ω–∏—Ü—ã: [-13.50, 86.50]
    
    Driving_License:
    –í—ã–±—Ä–æ—Å–æ–≤: 22757 (0.20%)
    –ì—Ä–∞–Ω–∏—Ü—ã: [1.00, 1.00]
    
    Region_Code:
    –í—ã–±—Ä–æ—Å–æ–≤: 0 (0.00%)
    –ì—Ä–∞–Ω–∏—Ü—ã: [-15.00, 65.00]
    
    Previously_Insured:
    –í—ã–±—Ä–æ—Å–æ–≤: 0 (0.00%)
    –ì—Ä–∞–Ω–∏—Ü—ã: [-1.50, 2.50]
    
    Annual_Premium:
    –í—ã–±—Ä–æ—Å–æ–≤: 2377273 (20.66%)
    –ì—Ä–∞–Ω–∏—Ü—ã: [4016.00, 60712.00]
    
    Policy_Sales_Channel:
    –í—ã–±—Ä–æ—Å–æ–≤: 0 (0.00%)
    –ì—Ä–∞–Ω–∏—Ü—ã: [-155.50, 336.50]
    
    Vintage:
    –í—ã–±—Ä–æ—Å–æ–≤: 0 (0.00%)
    –ì—Ä–∞–Ω–∏—Ü—ã: [-100.50, 431.50]
    


    
![png](readme_files/readme_14_1.png)
    


### 3.4. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

–ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö. –ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑–∞–ª, —á—Ç–æ –≤–æ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.


```python
missing_data = train_data.isnull().sum()
print(missing_data)

```

    id                      0
    Gender                  0
    Age                     0
    Driving_License         0
    Region_Code             0
    Previously_Insured      0
    Vehicle_Age             0
    Vehicle_Damage          0
    Annual_Premium          0
    Policy_Sales_Channel    0
    Vintage                 0
    Response                0
    dtype: int64
    

### 3.5. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

–°–º–æ—Ç—Ä–∏–º –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–∞–∂–Ω—ã. –î–ª—è —ç—Ç–æ–≥–æ —Å—á–∏—Ç–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–∞—é—Ç –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ù–∞–∏–±–æ–ª–µ–µ —Å–∏–ª—å–Ω–∞—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —É –ø—Ä–∏–∑–Ω–∞–∫–∞ Previously_Insured (-0.3459), —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç —á—Ç–æ –∫–ª–∏–µ–Ω—Ç—ã –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –∑–∞—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω—ã —Ä–µ–∂–µ —Å–æ–≥–ª–∞—à–∞—é—Ç—Å—è –Ω–∞ –Ω–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ. –ü—Ä–∏–∑–Ω–∞–∫ Policy_Sales_Channel —Ç–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é (-0.1527). –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è —É –ø—Ä–∏–∑–Ω–∞–∫–∞ Age (0.1221), —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º —á—Ç–æ —Å –≤–æ–∑—Ä–∞—Å—Ç–æ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ç–∫–ª–∏–∫–∞ –Ω–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è. –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–º–µ—é—Ç —Å–ª–∞–±—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.


```python
correlations = train_data[numeric_cols + [target_col]].corr()[target_col].sort_values(ascending=False)
target_correlations = correlations.drop(target_col)

plt.figure(figsize=(10, 6))
colors = ['red' if x < 0 else 'green' for x in target_correlations.values]
plt.barh(target_correlations.index, target_correlations.values, color=colors, alpha=0.7)
plt.xlabel('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å Response')
plt.title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π')
plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig(OTHER_DIR / 'feature_importance_correlations.png', dpi=150, bbox_inches='tight')
plt.show()

top_features = target_correlations.abs().sort_values(ascending=False).head(10)
for idx, (feature, corr) in enumerate(top_features.items(), 1):
    print(f"{idx}. {feature}: {target_correlations[feature]:.4f}")
```


    
![png](readme_files/readme_18_0.png)
    


    1. Previously_Insured: -0.3459
    2. Policy_Sales_Channel: -0.1527
    3. Age: 0.1221
    4. Annual_Premium: 0.0323
    5. Vintage: -0.0152
    6. Region_Code: 0.0128
    7. Driving_License: 0.0092
    

### 3.6. –ê–Ω–∞–ª–∏–∑ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

–°–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –†–µ—à–∞–µ–º –Ω—É–∂–Ω–æ –ª–∏ –∏—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å –∏ –º–æ–∂–Ω–æ –ª–∏ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏. –ü—Ä–∏–∑–Ω–∞–∫ Annual_Premium –∏–º–µ–µ—Ç —Å–∫–æ—à–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å –±–æ–ª—å—à–∏–º —Ä–∞–∑–±—Ä–æ—Å–æ–º –∑–Ω–∞—á–µ–Ω–∏–π (–æ—Ç 2,630 –¥–æ 540,165), –ø–æ—ç—Ç–æ–º—É –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ. –í–æ–∑—Ä–∞—Å—Ç –º–æ–∂–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –≥—Ä—É–ø–ø—ã (–º–æ–ª–æ–¥—ã–µ, —Å—Ä–µ–¥–Ω–∏–µ, –ø–æ–∂–∏–ª—ã–µ). –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –≤–∞–∂–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä Age * Annual_Premium –∏–ª–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ Vehicle_Age –∏ Vehicle_Damage.


```python
print("1. Age")
print(f"Min: {train_data['Age'].min()}")
print(f"Max: {train_data['Age'].max()}")
print(f"Mean: {train_data['Age'].mean():.2f}\n")

print("2. Annual_Premium")
print(f"Min: {train_data['Annual_Premium'].min()}")
print(f"Max: {train_data['Annual_Premium'].max()}")
print(f"Mean: {train_data['Annual_Premium'].mean():.2f}")
```

    1. Age
    Min: 20
    Max: 85
    Mean: 38.38
    
    2. Annual_Premium
    Min: 2630.0
    Max: 540165.0
    Mean: 30461.37
    

### 3.7. –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏

–ò–∑—É—á–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ –∏—Ö —Å–≤—è–∑—å —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–º–µ—é—Ç —Å–ª–∞–±—É—é —Å–≤—è–∑—å –º–µ–∂–¥—É —Å–æ–±–æ–π, —á—Ç–æ —Ö–æ—Ä–æ—à–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —è–≤–ª—è—é—Ç—Å—è –ø—Ä–∏–∑–Ω–∞–∫–∏ Previously_Insured, Policy_Sales_Channel –∏ Age, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞–∏–±–æ–ª—å—à—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π Response.


```python
correlation_matrix = train_data[numeric_cols + [target_col]].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
            center=0, square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
plt.tight_layout()
plt.savefig(OTHER_DIR / 'correlation_heatmap.png', dpi=150, bbox_inches='tight')
plt.show()

target_correlations = correlation_matrix[target_col].drop(target_col).sort_values(ascending=False)
print("\n–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:")
print("="*60)
print(target_correlations)
```


    
![png](readme_files/readme_22_0.png)
    


    
    –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:
    ============================================================
    Age                     0.122134
    Annual_Premium          0.032261
    Region_Code             0.012816
    Driving_License         0.009197
    Vintage                -0.015177
    Policy_Sales_Channel   -0.152733
    Previously_Insured     -0.345930
    Name: Response, dtype: float64
    

### 3.8. –û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã EDA

–ü–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –º—ã –≤—ã—è—Å–Ω–∏–ª–∏ —Å–ª–µ–¥—É—é—â–µ–µ:

1. **–î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤**: –ö–ª–∞—Å—Å—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –Ω–µ—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ - –∫–ª–∞—Å—Å 0 —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 87.70% (10,089,739 –Ω–∞–±–ª—é–¥–µ–Ω–∏–π), –∫–ª–∞—Å—Å 1 —Ç–æ–ª—å–∫–æ 12.30% (1,415,059 –Ω–∞–±–ª—é–¥–µ–Ω–∏–π). –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ 7.13:1. –ù—É–∂–Ω–æ —É—á–µ—Å—Ç—å —ç—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É—è —Ç–µ—Ö–Ω–∏–∫–∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤.

2. **–í–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏**: –ù–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ —è–≤–ª—è—é—Ç—Å—è Previously_Insured (–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è -0.3459), Policy_Sales_Channel (-0.1527) –∏ Age (0.1221). –≠—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–∏–ª—å–Ω–µ–µ –≤—Å–µ–≥–æ —Å–≤—è–∑–∞–Ω—ã —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.

3. **–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö**: –í –¥–∞–Ω–Ω—ã—Ö 7 —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ 3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö.

4. **–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è**: –ü—Ä–∏–∑–Ω–∞–∫ Annual_Premium –∏–º–µ–µ—Ç —Å–∫–æ—à–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å –±–æ–ª—å—à–∏–º —Ä–∞–∑–±—Ä–æ—Å–æ–º –∑–Ω–∞—á–µ–Ω–∏–π, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ.

5. **–í—ã–±—Ä–æ—Å—ã**: –ù–∞–π–¥–µ–Ω—ã –≤—ã–±—Ä–æ—Å—ã –≤ Annual_Premium (20.66% –¥–∞–Ω–Ω—ã—Ö) –∏ Driving_License (0.20%). –í—ã–±—Ä–æ—Å—ã –≤ Annual_Premium —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏.

6. **–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**: –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –≤—Å–µ 11,504,798 –Ω–∞–±–ª—é–¥–µ–Ω–∏–π —Å–æ–¥–µ—Ä–∂–∞—Ç –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

7. **–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è**: –ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ Annual_Premium, –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è Age, —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –≤–∞–∂–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏.

## 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è

–î–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ train –∏ validation —Å–æ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–µ–π, –ø–æ—Ç–æ–º—É —á—Ç–æ –∫–ª–∞—Å—Å—ã –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã –∏ –Ω–∞–º –≤–∞–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Ö –¥–æ–ª–∏. –ë–µ—Ä–µ–º 20 –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –ø–æ–¥ validation –∏ —Ñ–∏–∫—Å–∏—Ä—É–µ–º random_state=42 –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏. –£—Ç–µ—á–∫—É –∑–∞–∫—Ä—ã–≤–∞–µ–º —Ç–µ–º, —á—Ç–æ id –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫ –∏ –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø–æ—è–≤–ª—è–µ—Ç—Å—è –ø–æ—Å–ª–µ –º–æ–º–µ–Ω—Ç–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.


```python
X = train_data.drop([target_col, 'id'], axis=1)
y = train_data[target_col]

X_train, X_val, y_train, y_val = train_test_split(
    X, y, 
    test_size=TEST_SIZE, 
    random_state=RANDOM_STATE,
    stratify=y
)

print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
print(f"–†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_val.shape}")
print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ train:")
print(y_train.value_counts(normalize=True))
print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ validation:")
print(y_val.value_counts(normalize=True))
```

    –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: (9203838, 10)
    –†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: (2300960, 10)
    
    –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ train:
    Response
    0    0.877003
    1    0.122997
    Name: proportion, dtype: float64
    
    –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ validation:
    Response
    0    0.877003
    1    0.122997
    Name: proportion, dtype: float64
    

## 5. LightAutoML Baseline

–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –¥–µ–ª–∞–µ–º –±–∞–∑–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é LightAutoML. –ü—Ä–æ–≤–æ–¥–∏–º –¥–≤–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ —á—Ç–æ–±—ã –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç.

### 5.1. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 1: –ë–∞–∑–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

–°–Ω–∞—á–∞–ª–∞ –ø–æ—Å—Ç—Ä–æ–∏–º —Å–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π –±–µ–π–∑–ª–∞–π–Ω –Ω–∞ LightAutoML –±–µ–∑ —Å–µ—Ä—å–µ–∑–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∏ —É—á–∏–º—Å—è –Ω–∞ –≤—ã–±–æ—Ä–∫–µ 500–∫ —Å—Ç—Ä–æ–∫, –ø–æ—Ç–æ–º—É —á—Ç–æ –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π –∏ —Ç–∞–∫ –±—ã—Å—Ç—Ä–µ–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–¥–µ—é. –î–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—Ä–æ—Å—Å –≤ 5 —Ñ–æ–ª–¥–æ–≤ –∏ —Ñ–∏–∫—Å–∏—Ä—É–µ–º random_state.


```python
task = Task('binary', metric='auc')

roles = {
    'target': target_col,
    'drop': ['id']
}

train_lam = pd.concat([X_train, y_train], axis=1)
train_lam = train_lam.reset_index(drop=True)

sample_size = 500000
if len(train_lam) > sample_size:
    train_lam_sample = train_lam.sample(n=sample_size, random_state=RANDOM_STATE)
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ {sample_size} —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
else:
    train_lam_sample = train_lam

print("–û–±—É—á–µ–Ω–∏–µ LightAutoML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 1")
start_time = time.time()

automl1 = TabularAutoML(
    task=task,
    timeout=TIMEOUT,
    cpu_limit=N_THREADS,
    reader_params={'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},
    general_params={'use_algos': [['linear_l2', 'lgb', 'lgb_tuned', 'cb']]}
)

oof_pred1 = automl1.fit_predict(train_lam_sample, roles=roles, verbose=1)

train_time1 = time.time() - start_time
print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {train_time1:.2f} —Å–µ–∫—É–Ω–¥")

val_pred1 = automl1.predict(X_val.reset_index(drop=True))
val_pred1_proba = val_pred1.data[:, 0] if val_pred1.data.ndim > 1 else val_pred1.data

score1 = roc_auc_score(y_val.reset_index(drop=True), val_pred1_proba)
print(f"ROC-AUC –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 1: {score1:.6f}")

joblib.dump(automl1, MODELS_DIR / 'lama_config1.pkl')
print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODELS_DIR / 'lama_config1.pkl'}")
```

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ 500000 —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    –û–±—É—á–µ–Ω–∏–µ LightAutoML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 1
    [21:20:16] Stdout logging level is INFO.
    [21:20:16] Copying TaskTimer may affect the parent PipelineTimer, so copy will create new unlimited TaskTimer
    [21:20:16] Task: binary
    
    [21:20:16] Start automl preset with listed constraints:
    [21:20:16] - time: 1800.00 seconds
    [21:20:16] - CPU: 16 cores
    [21:20:16] - memory: 16 GB
    
    [21:20:16] [1mTrain data shape: (500000, 11)[0m
    
    [21:20:36] Layer [1m1[0m train process start. Time left 1779.56 secs
    [21:20:44] Start fitting [1mLvl_0_Pipe_0_Mod_0_LinearL2[0m ...
    [21:21:04] Fitting [1mLvl_0_Pipe_0_Mod_0_LinearL2[0m finished. score = [1m0.8737296264083111[0m
    [21:21:04] [1mLvl_0_Pipe_0_Mod_0_LinearL2[0m fitting and predicting completed
    [21:21:04] Time left 1752.18 secs
    
    [21:21:06] [1mSelector_LightGBM[0m fitting and predicting completed
    [21:21:14] Start fitting [1mLvl_0_Pipe_1_Mod_0_LightGBM[0m ...
    [21:21:24] Fitting [1mLvl_0_Pipe_1_Mod_0_LightGBM[0m finished. score = [1m0.8686815680853346[0m
    [21:21:24] [1mLvl_0_Pipe_1_Mod_0_LightGBM[0m fitting and predicting completed
    [21:21:24] Start hyperparameters optimization for [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m ... Time budget is 300.00 secs
    

    Optimization Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [03:56<00:00,  2.34s/it, best_trial=100, best_value=0.871]

    [21:25:21] Hyperparameters optimization for [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m completed
    [21:25:21] Start fitting [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m ...
    

    
    

    [21:25:34] Fitting [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m finished. score = [1m0.8696492846291531[0m
    [21:25:34] [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m fitting and predicting completed
    [21:25:34] Start fitting [1mLvl_0_Pipe_1_Mod_2_CatBoost[0m ...
    [21:26:24] Fitting [1mLvl_0_Pipe_1_Mod_2_CatBoost[0m finished. score = [1m0.8685317708587297[0m
    [21:26:24] [1mLvl_0_Pipe_1_Mod_2_CatBoost[0m fitting and predicting completed
    [21:26:24] Time left 1432.17 secs
    
    [21:26:24] [1mLayer 1 training completed.[0m
    
    [21:26:24] Blending: optimization starts with equal weights. Score = [1m0.8739455[0m
    [21:26:26] Blending: iteration [1m0[0m: score = [1m0.8765821[0m, weights = [1m[0.6704342  0.14913028 0.18043557 0.        ][0m
    [21:26:28] Blending: iteration [1m1[0m: score = [1m0.8766603[0m, weights = [1m[0.6065783  0.15806907 0.23535265 0.        ][0m
    [21:26:30] Blending: iteration [1m2[0m: score = [1m0.8766666[0m, weights = [1m[0.6067139  0.14182718 0.25145894 0.        ][0m
    [21:26:32] Blending: iteration [1m3[0m: score = [1m0.8766715[0m, weights = [1m[0.6088653  0.12752432 0.26361036 0.        ][0m
    [21:26:34] Blending: iteration [1m4[0m: score = [1m0.8766744[0m, weights = [1m[0.6081248  0.11601679 0.27585837 0.        ][0m
    [21:26:34] Blending: best score = [1m0.8766744[0m, best weights = [1m[0.6081248  0.11601679 0.27585837 0.        ][0m
    [21:26:34] [1mAutoml preset training completed in 378.18 seconds[0m
    
    [21:26:34] Model description:
    Final prediction for new objects (level 0) = 
    	 0.60812 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +
    	 0.11602 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +
    	 0.27586 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) 
    
    –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ 378.23 —Å–µ–∫—É–Ω–¥
    ROC-AUC –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 1: 0.875940
    –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: src\models\lama_config1.pkl
    

### 5.2. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 2: –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

–í–æ –≤—Ç–æ—Ä–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å–∏–º LightAutoML –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –≤—ã–±–æ—Ä –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ —É–≤–µ–ª–∏—á–∏—Ç—å –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–∂–µ—Ç –≤—ã—Ä–∞—Å—Ç–∏. –≠—Ç–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å, –µ—Å—Ç—å –ª–∏ —Å–º—ã—Å–ª —É—Å–ª–æ–∂–Ω—è—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫—É.


```python
print("–û–±—É—á–µ–Ω–∏–µ LightAutoML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 2")
start_time = time.time()

automl2 = TabularAutoML(
    task=task,
    timeout=TIMEOUT * 2,
    cpu_limit=N_THREADS,
    reader_params={
        'n_jobs': N_THREADS, 
        'cv': N_FOLDS, 
        'random_state': RANDOM_STATE
    },
    general_params={
        'use_algos': [['linear_l2', 'lgb', 'lgb_tuned']]
    },
    tuning_params={
        'max_tuning_iter': 250,
        'max_tuning_time': 800,
        'fit_on_holdout': True
    },
    selection_params={
        'mode': 1,
        'importance_type': 'gain',
        'fit_on_holdout': True
    }
)
oof_pred2 = automl2.fit_predict(train_lam_sample, roles=roles, verbose=1)

train_time2 = time.time() - start_time
print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {train_time2:.2f} —Å–µ–∫—É–Ω–¥")

val_pred2 = automl2.predict(X_val.reset_index(drop=True))
val_pred2_proba = val_pred2.data[:, 0] if val_pred2.data.ndim > 1 else val_pred2.data

score2 = roc_auc_score(y_val.reset_index(drop=True), val_pred2_proba)
print(f"ROC-AUC –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 2: {score2:.6f}")

joblib.dump(automl2, MODELS_DIR / 'lama_config2.pkl')
print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODELS_DIR / 'lama_config2.pkl'}")
```

    –û–±—É—á–µ–Ω–∏–µ LightAutoML –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 2
    [11:06:04] Stdout logging level is INFO.
    [11:06:04] Task: binary
    
    [11:06:04] Start automl preset with listed constraints:
    [11:06:04] - time: 3600.00 seconds
    [11:06:04] - CPU: 16 cores
    [11:06:04] - memory: 16 GB
    
    [11:06:04] [1mTrain data shape: (500000, 11)[0m
    
    [11:06:11] Layer [1m1[0m train process start. Time left 3593.02 secs
    [11:06:18] Start fitting [1mLvl_0_Pipe_0_Mod_0_LinearL2[0m ...
    [11:06:37] Fitting [1mLvl_0_Pipe_0_Mod_0_LinearL2[0m finished. score = [1m0.8737668079133237[0m
    [11:06:37] [1mLvl_0_Pipe_0_Mod_0_LinearL2[0m fitting and predicting completed
    [11:06:37] Time left 3566.50 secs
    
    [11:06:39] [1mSelector_LightGBM[0m fitting and predicting completed
    [11:06:47] Start fitting [1mLvl_0_Pipe_1_Mod_0_LightGBM[0m ...
    [11:06:59] Fitting [1mLvl_0_Pipe_1_Mod_0_LightGBM[0m finished. score = [1m0.8686815680853346[0m
    [11:06:59] [1mLvl_0_Pipe_1_Mod_0_LightGBM[0m fitting and predicting completed
    [11:06:59] Start hyperparameters optimization for [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m ... Time budget is 800.00 secs
    

    Optimization Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [11:53<00:00,  2.86s/it, best_trial=209, best_value=0.871]

    [11:18:52] Hyperparameters optimization for [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m completed
    [11:18:52] Start fitting [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m ...
    

    
    

    [11:19:20] Fitting [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m finished. score = [1m0.8697426091068572[0m
    [11:19:20] [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m fitting and predicting completed
    [11:19:20] Time left 2803.37 secs
    
    [11:19:20] [1mLayer 1 training completed.[0m
    
    [11:19:20] Blending: optimization starts with equal weights. Score = [1m0.8750297[0m
    [11:19:22] Blending: iteration [1m0[0m: score = [1m0.8766706[0m, weights = [1m[0.60501844 0.17357859 0.22140294][0m
    [11:19:23] Blending: iteration [1m1[0m: score = [1m0.8766806[0m, weights = [1m[0.6064344  0.15261787 0.24094774][0m
    [11:19:25] Blending: iteration [1m2[0m: score = [1m0.8766876[0m, weights = [1m[0.60735625 0.1346268  0.25801688][0m
    [11:19:26] Blending: iteration [1m3[0m: score = [1m0.8766911[0m, weights = [1m[0.6073863  0.12298153 0.2696322 ][0m
    [11:19:28] Blending: iteration [1m4[0m: score = [1m0.8766941[0m, weights = [1m[0.6080522  0.11170325 0.28024453][0m
    [11:19:28] Blending: best score = [1m0.8766941[0m, best weights = [1m[0.6080522  0.11170325 0.28024453][0m
    [11:19:28] [1mAutoml preset training completed in 804.32 seconds[0m
    
    [11:19:28] Model description:
    Final prediction for new objects (level 0) = 
    	 0.60805 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +
    	 0.11170 * (5 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +
    	 0.28024 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) 
    
    –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ 804.34 —Å–µ–∫—É–Ω–¥
    ROC-AUC –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è 2: 0.875985
    –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: src\lama_config2.pkl
    

### 5.3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π LightAutoML

–°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ–±–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ ROC AUC, –ø–æ—Ç–æ–º—É —á—Ç–æ —ç—Ç–æ –º–µ—Ç—Ä–∏–∫–∞ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è. –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å–º–æ—Ç—Ä–∏–º –Ω–∞ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ –¥–æ—Ä–æ–≥–æ –æ–±—Ö–æ–¥–∏—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–µ. –ò–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∏–¥–Ω–æ —á—Ç–æ —É—Å–ª–æ–∂–Ω–∏–≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –Ω–µ—Ç, —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏–ª–æ—Å—å –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, –ø–æ—ç—Ç–æ–º—É –≤—ã–±–µ—Ä–µ–º –ø—Ä–æ—Å—Ç–æ–π –≤–∞—Ä–∏–∞–Ω—Ç.


```python
results_comparison = pd.DataFrame({
    '–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è': ['LightAutoML Config 1', 'LightAutoML Config 2'],
    'ROC-AUC': [score1, score2],
    '–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (—Å–µ–∫)': [train_time1, train_time2]
})

print(results_comparison)

best_automl = automl1 if score1 >= score2 else automl2
best_score = max(score1, score2)
best_config = 'Config 1' if score1 >= score2 else 'Config 2'

print(f"\n–õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {best_config}")
print(f"–õ—É—á—à–∏–π ROC-AUC: {best_score:.6f}")

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].bar(['Config 1', 'Config 2'], [score1, score2], color=['skyblue', 'salmon'])
axes[0].set_ylabel('ROC-AUC')
axes[0].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ ROC-AUC')
axes[0].set_ylim([min(score1, score2) - 0.01, max(score1, score2) + 0.01])
axes[0].grid(True, alpha=0.3, axis='y')
for i, v in enumerate([score1, score2]):
    axes[0].text(i, v, f'{v:.6f}', ha='center', va='bottom')

axes[1].bar(['Config 1', 'Config 2'], [train_time1, train_time2], color=['lightgreen', 'orange'])
axes[1].set_ylabel('–í—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)')
axes[1].set_title('–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è')
axes[1].grid(True, alpha=0.3, axis='y')
for i, v in enumerate([train_time1, train_time2]):
    axes[1].text(i, v, f'{v:.1f}—Å', ha='center', va='bottom')

plt.tight_layout()
plt.savefig(OTHER_DIR / 'baseline_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

baseline_metrics = {
    'config1_auc': float(score1),
    'config2_auc': float(score2),
    'best_config': best_config,
    'best_auc': float(best_score)
}
import json
with open(PARAMS_DIR / 'baseline_metrics.json', 'w') as f:
    json.dump(baseline_metrics, f, indent=2)
```

               –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è   ROC-AUC  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (—Å–µ–∫)
    0  LightAutoML Config 1  0.875943            365.300634
    1  LightAutoML Config 2  0.875985            804.338054
    
    –õ—É—á—à–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: Config 2
    –õ—É—á—à–∏–π ROC-AUC: 0.875985
    


    
![png](readme_files/readme_32_1.png)
    


### 5.4. –û–±—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö

–ü–æ—Å–ª–µ –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±—É—á–∞–µ–º –µ–µ –Ω–∞ –≤—Å–µ–º train, –ø–æ—Ç–æ–º—É —á—Ç–æ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –≤–∞–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞–∫—Å–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö. –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏, –∏–Ω–∞—á–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç –Ω–µ —É—Å–ø–µ—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è.


```python
print("–û–±—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ LightAutoML –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö")

train_data_full = pd.read_csv(DATA_DIR / 'train.csv')
train_lam_full = train_data_full.drop(['id'], axis=1, errors='ignore')
train_lam_full = train_lam_full.reset_index(drop=True)

print(f"–†–∞–∑–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(train_lam_full)} —Å—Ç—Ä–æ–∫")

print("–û–±—É—á–µ–Ω–∏–µ LightAutoML –Ω–∞ –ø–æ–ª–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ")
start_time = time.time()

# –±–µ–∑ cb, –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –≤—Å–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ(
automl_final = TabularAutoML(
    task=task,
    timeout=TIMEOUT*5,
    cpu_limit=N_THREADS,
    reader_params={'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},
    general_params={'use_algos': [['linear_l2', 'lgb', 'lgb_tuned']]}
)

oof_pred_final = automl_final.fit_predict(train_lam_full, roles=roles, verbose=1)

train_time_final = time.time() - start_time
print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {train_time_final:.2f} —Å–µ–∫—É–Ω–¥")

joblib.dump(automl_final, MODELS_DIR / 'lama_final.pkl')
print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODELS_DIR / 'lama_final.pkl'}")
```

    –û–±—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ LightAutoML –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
    –†–∞–∑–º–µ—Ä –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: 11504798 —Å—Ç—Ä–æ–∫
    –û–±—É—á–µ–Ω–∏–µ LightAutoML –Ω–∞ –ø–æ–ª–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
    [12:19:19] Stdout logging level is INFO.
    [12:19:19] Task: binary
    
    [12:19:19] Start automl preset with listed constraints:
    [12:19:19] - time: 9000.00 seconds
    [12:19:19] - CPU: 16 cores
    [12:19:19] - memory: 16 GB
    
    [12:19:19] [1mTrain data shape: (11504798, 11)[0m
    
    [12:19:31] Layer [1m1[0m train process start. Time left 8988.67 secs
    [12:22:20] Start fitting [1mLvl_0_Pipe_0_Mod_0_LinearL2[0m ...
    [12:26:51] Fitting [1mLvl_0_Pipe_0_Mod_0_LinearL2[0m finished. score = [1m0.8747224795998259[0m
    [12:26:51] [1mLvl_0_Pipe_0_Mod_0_LinearL2[0m fitting and predicting completed
    [12:26:51] Time left 8548.21 secs
    
    [12:30:13] [1mSelector_LightGBM[0m fitting and predicting completed
    [12:33:22] Start fitting [1mLvl_0_Pipe_1_Mod_0_LightGBM[0m ...
    [12:38:54] Fitting [1mLvl_0_Pipe_1_Mod_0_LightGBM[0m finished. score = [1m0.8741067416178282[0m
    [12:38:54] [1mLvl_0_Pipe_1_Mod_0_LightGBM[0m fitting and predicting completed
    [12:38:54] Start hyperparameters optimization for [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m ... Time budget is 300.00 secs
    

    Optimization Progress:   2%|‚ñè         | 2/101 [05:33<4:35:30, 166.98s/it, best_trial=1, best_value=0.875]

    [12:44:28] Hyperparameters optimization for [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m completed
    [12:44:28] Start fitting [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m ...
    

    
    

    [12:59:20] Fitting [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m finished. score = [1m0.8744681275340629[0m
    [12:59:20] [1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM[0m fitting and predicting completed
    [12:59:20] Time left 6599.45 secs
    
    [12:59:20] [1mLayer 1 training completed.[0m
    
    [12:59:23] Blending: optimization starts with equal weights. Score = [1m0.8786178[0m
    [13:00:22] Blending: iteration [1m0[0m: score = [1m0.8794030[0m, weights = [1m[0.5081756  0.19243884 0.29938555][0m
    [13:01:22] Blending: iteration [1m1[0m: score = [1m0.8794454[0m, weights = [1m[0.51290786 0.14103885 0.34605327][0m
    [13:02:21] Blending: iteration [1m2[0m: score = [1m0.8794808[0m, weights = [1m[0.5169587  0.09268539 0.39035589][0m
    [13:03:21] Blending: iteration [1m3[0m: score = [1m0.8795022[0m, weights = [1m[0.5195201  0.06025551 0.42022434][0m
    [13:04:21] Blending: iteration [1m4[0m: score = [1m0.8795368[0m, weights = [1m[0.52373666 0.         0.47626334][0m
    [13:04:21] Blending: best score = [1m0.8795368[0m, best weights = [1m[0.52373666 0.         0.47626334][0m
    [13:04:21] [1mAutoml preset training completed in 2701.73 seconds[0m
    
    [13:04:21] Model description:
    Final prediction for new objects (level 0) = 
    	 0.52374 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +
    	 0.47626 * (5 averaged models Lvl_0_Pipe_1_Mod_1_Tuned_LightGBM) 
    
    –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ 2701.75 —Å–µ–∫—É–Ω–¥
    –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: src\lama_final.pkl
    

### 5.5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è submission –¥–ª—è baseline

–î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ test –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ö –≤ CSV. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω—É—é –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏.
–í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –Ω–∞ –∑–∞–∫—Ä—ã—Ç—ã—Ö —Ç–µ—Å—Ç–∞—Ö –±–µ–π–∑–ª–∞–π–Ω –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–ª–∞ —Ö–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç `0.87945`, –ª—É—á—à–µ —á–µ–º —É –ø—Ä–∏–º–µ—Ä–Ω–æ –ø–æ–ª–æ–≤–∏–Ω—ã —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ 


```python
test_data_full = pd.read_csv(DATA_DIR / 'test.csv')
print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(test_data_full)}")

test_pred = automl_final.predict(test_data_full)
test_pred_proba = test_pred.data[:, 0] if test_pred.data.ndim > 1 else test_pred.data

submission_baseline = pd.DataFrame({
    'id': test_data_full['id'].values,
    'Response': test_pred_proba
})

submission_file_baseline = SUBMIT_DIR / 'baseline_submission.csv'
submission_baseline.to_csv(submission_file_baseline, index=False)

print(f"Submission —Å–æ–∑–¥–∞–Ω: {submission_file_baseline}")
print(f"–†–∞–∑–º–µ—Ä: {submission_baseline.shape}")
print(f"–î–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: [{test_pred_proba.min():.6f}, {test_pred_proba.max():.6f}]")
print(f"–°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {test_pred_proba.mean():.6f}")
```

    –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: 7669866
    Submission —Å–æ–∑–¥–∞–Ω: src\baseline_submission.csv
    –†–∞–∑–º–µ—Ä: (7669866, 2)
    –î–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: [0.000010, 0.929261]
    –°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: 0.122994
    

## 6. –°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –±–µ–∑ LightAutoML

–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –±–µ–∑ LightAutoML. –î–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –æ—Ç –ø—Ä–æ—Å—Ç—ã—Ö –∫ —Å–ª–æ–∂–Ω—ã–º —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –ª—É—á—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ. –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –±–ª–æ–∫–Ω–æ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ `src/`.

### 6.1. Pipeline –≤–µ—Ä—Å–∏–∏ 1: –ë–∞–∑–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥

–ù–∞—á–∏–Ω–∞–µ–º —Å –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –Ω–∞ RandomForest, –¥–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä—É –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ EDA, –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä—É–µ–º Annual_Premium –∏–∑-–∑–∞ —Å–∏–ª—å–Ω–æ–π –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏, –¥–µ–ª–∞–µ–º –≥—Ä—É–ø–ø—ã –ø–æ –≤–æ–∑—Ä–∞—Å—Ç—É –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–∞ —Å Premium, –ø–æ—Ç–æ–º—É —á—Ç–æ –≤–º–µ—Å—Ç–µ —ç—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–∞—Å—Ç–æ –Ω–µ—Å—É—Ç –±–æ–ª—å—à–µ —Å–º—ã—Å–ª–∞, —á–µ–º –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏. –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ–¥–∏—Ä—É–µ–º —á–∏—Å–ª–∞–º–∏, –ø—Ä–æ–ø—É—Å–∫–∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –º–µ–¥–∏–∞–Ω–æ–π, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã RandomForest –±–µ—Ä–µ–º —É–º–µ—Ä–µ–Ω–Ω—ã–µ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–µ–Ω—å—à–µ –ø–µ—Ä–µ–æ–±—É—á–∞–ª–∞—Å—å –∏ —Ä–∞–±–æ—Ç–∞–ª–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ.


```python
def create_features_v1(df):
    df = df.copy()
    
    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä–æ–≤–∞–Ω–∏–µ
    if 'Annual_Premium' in df.columns:
        df['Annual_Premium_log'] = np.log1p(df['Annual_Premium'])
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
    if 'Age' in df.columns:
        df['Age_group'] = pd.cut(df['Age'], 
                                bins=[0, 25, 35, 45, 55, 100],
                                labels=['18-25', '26-35', '36-45', '46-55', '55+'])
        df['Age_group'] = df['Age_group'].astype(str)
    
    # –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ Age –∏ Annual_Premium
    if 'Age' in df.columns and 'Annual_Premium' in df.columns:
        df['Age_Premium_interaction'] = df['Age'] * df['Annual_Premium'] / 1000
    
    return df

def preprocess_data_v1(X_train, X_val):
    X_train_proc = create_features_v1(X_train)
    X_val_proc = create_features_v1(X_val)
    
    categorical_cols = X_train_proc.select_dtypes(include=['object']).columns.tolist()
    
    # Label Encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
    label_encoders = {}
    for col in categorical_cols:
        le = LabelEncoder()
        X_train_proc[col] = le.fit_transform(X_train_proc[col].astype(str))
        X_val_proc[col] = le.transform(X_val_proc[col].astype(str))
        label_encoders[col] = le
    
    # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    numeric_cols = X_train_proc.select_dtypes(include=[np.number]).columns.tolist()
    for col in numeric_cols:
        median_val = X_train_proc[col].median()
        X_train_proc[col].fillna(median_val, inplace=True)
        X_val_proc[col].fillna(median_val, inplace=True)
    
    return X_train_proc, X_val_proc, label_encoders

print("–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö Pipeline v1")
X_train_proc_v1, X_val_proc_v1, encoders_v1 = preprocess_data_v1(X_train, X_val)

sample_size = 2000000
if len(X_train_proc_v1) > sample_size:
    sample_idx = np.random.choice(len(X_train_proc_v1), sample_size, replace=False)
    X_train_sample_v1 = X_train_proc_v1.iloc[sample_idx]
    y_train_sample_v1 = y_train.iloc[sample_idx]
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ {sample_size} —Å—Ç—Ä–æ–∫")
else:
    X_train_sample_v1 = X_train_proc_v1
    y_train_sample_v1 = y_train

print("–û–±—É—á–µ–Ω–∏–µ Pipeline v1")
start_time = time.time()

pipeline_v1 = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(
        n_estimators=150,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=RANDOM_STATE,
        n_jobs=N_THREADS,
        verbose=0
    ))
])

pipeline_v1.fit(X_train_sample_v1, y_train_sample_v1)

train_time_v1 = time.time() - start_time
print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {train_time_v1:.2f} —Å–µ–∫—É–Ω–¥")

val_pred_proba_v1 = pipeline_v1.predict_proba(X_val_proc_v1)[:, 1]
score_v1 = roc_auc_score(y_val, val_pred_proba_v1)

print(f"ROC-AUC Pipeline v1: {score_v1:.6f}")

joblib.dump(pipeline_v1, MODELS_DIR / 'pipeline_v1.pkl')
print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODELS_DIR / 'pipeline_v1.pkl'}")
```

    –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö Pipeline v1
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ 2000000 —Å—Ç—Ä–æ–∫
    –û–±—É—á–µ–Ω–∏–µ Pipeline v1
    –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ 31.71 —Å–µ–∫—É–Ω–¥
    ROC-AUC Pipeline v1: 0.860628
    –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: src\models\pipeline_v1.pkl
    

–†–µ–∑—É–ª—å—Ç–∞—Ç `0.86063` —á—É—Ç—å —Ö—É–∂–µ —á–µ–º `0.87945` —É LightAutoML

### 6.2. Pipeline –≤–µ—Ä—Å–∏–∏ 2: LightGBM –ø–æ–¥—Ö–æ–¥

–í–æ –≤—Ç–æ—Ä–æ–π –ø–æ–ø—ã—Ç–∫–µ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –±—É—Å—Ç–∏–Ω–≥, –ø–æ—Ç–æ–º—É —á—Ç–æ –¥–ª—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–Ω —á–∞—Å—Ç–æ –¥–∞–µ—Ç –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —á–µ–º RandomForest. –ü–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –¥–µ–ª–∞–µ–º –±–æ–ª–µ–µ –∞–∫–∫—É—Ä–∞—Ç–Ω—É—é –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö: —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –ø—Ä–æ–±—É–µ–º –º—è–≥–∫–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —è–≤–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥—Å–∫–∞–∑–∞–ª EDA. –î–∞–ª—å—à–µ –æ–±—É—á–∞–µ–º LightGBM –∏ —É—á–∏—Ç—ã–≤–∞–µ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤.


```python
print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Pipeline v2")

import lightgbm as lgb

model_path = MODELS_DIR / 'model_v2_lgb.pkl'

def remove_duplicates(X, y=None):
    initial_size = len(X)
    if y is not None:
        X_with_target = X.copy()
        X_with_target['_target'] = y.values
        X_with_target = X_with_target.drop_duplicates()
        y_cleaned = pd.Series(X_with_target['_target'].values, index=X_with_target.index)
        X_cleaned = X_with_target.drop('_target', axis=1)
        duplicates_removed = initial_size - len(X_cleaned)
        if duplicates_removed > 0:
            print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed:,} ({duplicates_removed/initial_size*100:.2f}%)")
        return X_cleaned, y_cleaned
    else:
        X_cleaned = X.drop_duplicates()
        duplicates_removed = initial_size - len(X_cleaned)
        if duplicates_removed > 0:
            print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed:,} ({duplicates_removed/initial_size*100:.2f}%)")
        return X_cleaned

def detect_and_remove_anomalies(X, y=None, z_threshold=4.0, iqr_factor=2.5):
    initial_size = len(X)
    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
    anomaly_indices = set()
    
    for col in numeric_cols:
        col_data = X[col].dropna()
        if len(col_data) == 0:
            continue
        z_scores = np.abs(stats.zscore(col_data))
        z_outliers = col_data.index[z_scores > z_threshold].tolist()
        Q1 = col_data.quantile(0.25)
        Q3 = col_data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - iqr_factor * IQR
        upper_bound = Q3 + iqr_factor * IQR
        iqr_outliers = col_data.index[(col_data < lower_bound) | (col_data > upper_bound)].tolist()
        outliers = set(z_outliers) & set(iqr_outliers)
        anomaly_indices.update(outliers)
    
    if len(anomaly_indices) > 0:
        anomaly_pct = len(anomaly_indices) / initial_size * 100
        if anomaly_pct < 5.0:
            X_cleaned = X.drop(index=anomaly_indices)
            if y is not None:
                y_cleaned = y.drop(index=anomaly_indices)
                print(f"–£–¥–∞–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {len(anomaly_indices):,} ({anomaly_pct:.2f}%)")
                return X_cleaned, y_cleaned
            else:
                print(f"–£–¥–∞–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {len(anomaly_indices):,} ({anomaly_pct:.2f}%)")
                return X_cleaned
    
    if y is not None:
        return X, y
    else:
        return X

def create_features_improved(df):
    df = df.copy()
    if 'Annual_Premium' in df.columns:
        df['Annual_Premium_log'] = np.log1p(df['Annual_Premium'])
        df['Annual_Premium_sqrt'] = np.sqrt(df['Annual_Premium'])
    if 'Age' in df.columns:
        df['Age_group'] = pd.cut(df['Age'], bins=[0, 25, 35, 45, 55, 100], labels=['18-25', '26-35', '36-45', '46-55', '55+'])
        df['Age_group'] = df['Age_group'].astype(str)
        df['Age_squared'] = df['Age'] ** 2
        df['Age_cubed'] = df['Age'] ** 3
    if 'Age' in df.columns and 'Annual_Premium' in df.columns:
        df['Age_Premium_interaction'] = df['Age'] * df['Annual_Premium'] / 1000
        df['Age_Premium_ratio'] = df['Annual_Premium'] / (df['Age'] + 1)
    if 'Vehicle_Age' in df.columns and 'Vehicle_Damage' in df.columns:
        df['Vehicle_combination'] = df['Vehicle_Age'].astype(str) + '_' + df['Vehicle_Damage'].astype(str)
    if 'Previously_Insured' in df.columns and 'Vehicle_Damage' in df.columns:
        df['Insured_Damage'] = df['Previously_Insured'].astype(str) + '_' + df['Vehicle_Damage'].astype(str)
    if 'Region_Code' in df.columns and 'Policy_Sales_Channel' in df.columns:
        df['Region_Channel'] = df['Region_Code'].astype(str) + '_' + df['Policy_Sales_Channel'].astype(str)
    return df

def preprocess_data_improved(X_train, X_val):
    X_train_proc = create_features_improved(X_train)
    X_val_proc = create_features_improved(X_val)
    
    categorical_cols = X_train_proc.select_dtypes(include=['object']).columns.tolist()
    numeric_cols = X_train_proc.select_dtypes(include=[np.number]).columns.tolist()
    
    label_encoders = {}
    for col in categorical_cols:
        le = LabelEncoder()
        X_train_proc[col] = le.fit_transform(X_train_proc[col].astype(str))
        X_val_proc[col] = X_val_proc[col].astype(str)
        known_classes = set(le.classes_)
        X_val_proc.loc[~X_val_proc[col].isin(known_classes), col] = le.classes_[0]
        X_val_proc[col] = le.transform(X_val_proc[col])
        label_encoders[col] = le
    
    for col in numeric_cols:
        median_val = X_train_proc[col].median()
        if pd.isna(median_val):
            median_val = 0.0
        X_train_proc[col] = X_train_proc[col].fillna(median_val)
        X_val_proc[col] = X_val_proc[col].fillna(median_val)
    
    return X_train_proc, X_val_proc

print("–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
X_train_clean, y_train_clean = remove_duplicates(X_train.copy(), y_train.copy())
X_train_clean, y_train_clean = detect_and_remove_anomalies(X_train_clean, y_train_clean)

X_train_proc, X_val_proc = preprocess_data_improved(X_train_clean, X_val.copy())

if model_path.exists():
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏")
    model_improved = joblib.load(model_path)
    
    val_pred_proba_improved = model_improved.predict(X_val_proc, num_iteration=model_improved.best_iteration)
    score_improved = roc_auc_score(y_val, val_pred_proba_improved)
    
    print(f"ROC-AUC –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å): {score_improved:.6f}")
else:
    print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")
    
    sample_size = 2000000
    if len(X_train_proc) > sample_size:
        sample_idx = np.random.choice(len(X_train_proc), sample_size, replace=False)
        X_train_sample = X_train_proc.iloc[sample_idx]
        y_train_sample = y_train_clean.iloc[sample_idx]
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ {sample_size} —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    else:
        X_train_sample = X_train_proc
        y_train_sample = y_train_clean
    
    train_data = lgb.Dataset(X_train_sample, label=y_train_sample)
    val_data = lgb.Dataset(X_val_proc, label=y_val, reference=train_data)
    
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': 50,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': 42,
        'scale_pos_weight': (y_train_sample == 0).sum() / (y_train_sample == 1).sum()
    }
    
    model_improved = lgb.train(
        params,
        train_data,
        valid_sets=[val_data],
        num_boost_round=500,
        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]
    )
    
    val_pred_proba_improved = model_improved.predict(X_val_proc, num_iteration=model_improved.best_iteration)
    score_improved = roc_auc_score(y_val, val_pred_proba_improved)
    print(f"ROC-AUC –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å): {score_improved:.6f}")
    
    joblib.dump(model_improved, MODELS_DIR / 'model_improved_optuna.pkl')

print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
print(f"Pipeline v1: {score_v1:.6f}")
if score_improved is not None:
    print(f"–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥: {score_improved:.6f}")
```

    –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Pipeline v2
    –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
      –£–¥–∞–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: 20,574 (0.22%)
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ 2000000 —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    ROC-AUC –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å): 0.877647
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    Pipeline v1: 0.860565
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥: 0.877647
    

–†–µ–∑—É–ª—å—Ç–∞—Ç `0.87765` –±–ª–∏–∑–æ–∫ –∫ `0.87945` —É LightAutoML, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –Ω–∞ –ø–æ—Ä—è–¥–æ–∫ –±—ã—Å—Ç—Ä–µ–µ

### 6.3. –£–ª—É—á—à–µ–Ω–Ω—ã–π LightGBM —Å –æ—á–∏—Å—Ç–∫–æ–π –¥–∞–Ω–Ω—ã—Ö –∏ Optuna

–ó–¥–µ—Å—å —Å–æ–±–∏—Ä–∞–µ–º –≤–æ–µ–¥–∏–Ω–æ –≤—Å–µ –∏–¥–µ–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–æ–≤ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ Optuna. –°–Ω–∞—á–∞–ª–∞ –æ—á–∏—â–∞–µ–º train –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ —è–≤–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π, –∑–∞—Ç–µ–º –¥–µ–ª–∞–µ–º feature engineering, –ø–æ—Å–ª–µ —á–µ–≥–æ –Ω–∞ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ–¥–≤—ã–±–æ—Ä–∫–µ –∑–∞–ø—É—Å–∫–∞–µ–º Optuna, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ —Ö–æ—Ä–æ—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã. –õ—É—á—à–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ –ø–æ–ª–Ω—ã–π –æ—á–∏—â–µ–Ω–Ω—ã–π train+val –∏ –æ–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å


```python
print("–£–ª—É—á—à–µ–Ω–Ω—ã–π LightGBM —Å Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π")

import lightgbm as lgb
import optuna
from sklearn.model_selection import train_test_split

model_path = MODELS_DIR / 'model_improved_optuna.pkl'

def remove_duplicates_optuna(X, y=None):
    initial_size = len(X)
    if y is not None:
        X_with_target = X.copy()
        X_with_target['_target'] = y.values
        X_with_target = X_with_target.drop_duplicates()
        y_cleaned = pd.Series(X_with_target['_target'].values, index=X_with_target.index)
        X_cleaned = X_with_target.drop('_target', axis=1)
        duplicates_removed = initial_size - len(X_cleaned)
        if duplicates_removed > 0:
            print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed:,} ({duplicates_removed/initial_size*100:.2f}%)")
        return X_cleaned, y_cleaned
    else:
        X_cleaned = X.drop_duplicates()
        duplicates_removed = initial_size - len(X_cleaned)
        if duplicates_removed > 0:
            print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed:,} ({duplicates_removed/initial_size*100:.2f}%)")
        return X_cleaned

def detect_and_remove_anomalies_optuna(X, y=None, z_threshold=4.0, iqr_factor=2.5):
    initial_size = len(X)
    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
    anomaly_indices = set()
    
    for col in numeric_cols:
        col_data = X[col].dropna()
        if len(col_data) == 0:
            continue
        z_scores = np.abs(stats.zscore(col_data))
        z_outliers = col_data.index[z_scores > z_threshold].tolist()
        Q1 = col_data.quantile(0.25)
        Q3 = col_data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - iqr_factor * IQR
        upper_bound = Q3 + iqr_factor * IQR
        iqr_outliers = col_data.index[(col_data < lower_bound) | (col_data > upper_bound)].tolist()
        outliers = set(z_outliers) & set(iqr_outliers)
        anomaly_indices.update(outliers)
    
    if len(anomaly_indices) > 0:
        anomaly_pct = len(anomaly_indices) / initial_size * 100
        if anomaly_pct < 5.0:
            X_cleaned = X.drop(index=anomaly_indices)
            if y is not None:
                y_cleaned = y.drop(index=anomaly_indices)
                print(f"–£–¥–∞–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {len(anomaly_indices):,} ({anomaly_pct:.2f}%)")
                return X_cleaned, y_cleaned
            else:
                print(f"–£–¥–∞–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {len(anomaly_indices):,} ({anomaly_pct:.2f}%)")
                return X_cleaned
    
    if y is not None:
        return X, y
    else:
        return X

def create_features_optuna(df):
    df = df.copy()
    if 'Annual_Premium' in df.columns:
        df['Annual_Premium_log'] = np.log1p(df['Annual_Premium'])
        df['Annual_Premium_sqrt'] = np.sqrt(df['Annual_Premium'])
    if 'Age' in df.columns:
        df['Age_group'] = pd.cut(df['Age'], bins=[0, 25, 35, 45, 55, 100], labels=['18-25', '26-35', '36-45', '46-55', '55+'])
        df['Age_group'] = df['Age_group'].astype(str)
        df['Age_squared'] = df['Age'] ** 2
        df['Age_cubed'] = df['Age'] ** 3
    if 'Age' in df.columns and 'Annual_Premium' in df.columns:
        df['Age_Premium_interaction'] = df['Age'] * df['Annual_Premium'] / 1000
        df['Age_Premium_ratio'] = df['Annual_Premium'] / (df['Age'] + 1)
    if 'Vehicle_Age' in df.columns and 'Vehicle_Damage' in df.columns:
        df['Vehicle_combination'] = df['Vehicle_Age'].astype(str) + '_' + df['Vehicle_Damage'].astype(str)
    if 'Previously_Insured' in df.columns and 'Vehicle_Damage' in df.columns:
        df['Insured_Damage'] = df['Previously_Insured'].astype(str) + '_' + df['Vehicle_Damage'].astype(str)
    if 'Region_Code' in df.columns and 'Policy_Sales_Channel' in df.columns:
        df['Region_Channel'] = df['Region_Code'].astype(str) + '_' + df['Policy_Sales_Channel'].astype(str)
    return df

def preprocess_data_optuna(X_train, X_val):
    X_train_proc = create_features_optuna(X_train)
    X_val_proc = create_features_optuna(X_val)
    
    categorical_cols = X_train_proc.select_dtypes(include=['object']).columns.tolist()
    numeric_cols = X_train_proc.select_dtypes(include=[np.number]).columns.tolist()
    
    label_encoders = {}
    for col in categorical_cols:
        le = LabelEncoder()
        X_train_proc[col] = le.fit_transform(X_train_proc[col].astype(str))
        X_val_proc[col] = X_val_proc[col].astype(str)
        known_classes = set(le.classes_)
        X_val_proc.loc[~X_val_proc[col].isin(known_classes), col] = le.classes_[0]
        X_val_proc[col] = le.transform(X_val_proc[col])
        label_encoders[col] = le
    
    for col in numeric_cols:
        median_val = X_train_proc[col].median()
        if pd.isna(median_val):
            median_val = 0.0
        X_train_proc[col] = X_train_proc[col].fillna(median_val)
        X_val_proc[col] = X_val_proc[col].fillna(median_val)
    
    return X_train_proc, X_val_proc, label_encoders

def coreset_sampling_stratified(X, y, n_samples=300000):
    if len(X) <= n_samples:
        print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö ({len(X):,}) <= —Ç—Ä–µ–±—É–µ–º–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ ({n_samples:,}), –≤—ã–±–æ—Ä–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
        return X, y
    
    _, X_sample, _, y_sample = train_test_split(
        X, y,
        train_size=n_samples,
        stratify=y,
        random_state=42
    )
    print(f"–°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_sample):,} —Å—Ç—Ä–æ–∫")
    return X_sample, y_sample

print("\n[1/4] –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
X_train_clean, y_train_clean = remove_duplicates_optuna(X_train.copy(), y_train.copy())
X_train_clean, y_train_clean = detect_and_remove_anomalies_optuna(X_train_clean, y_train_clean)

print("\n[2/4] –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
X_train_proc, X_val_proc, label_encoders = preprocess_data_optuna(X_train_clean, X_val.copy())

print("\n[3/4] –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è Optuna")
optuna_sample_size = 300000
X_train_optuna, y_train_optuna = coreset_sampling_stratified(X_train_proc, y_train_clean, n_samples=optuna_sample_size)

print("\n[4/4] –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å Optuna")

def objective(trial):
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 20, 150),
        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
        'min_child_samples': trial.suggest_int('min_child_samples', 5, 200),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
        'max_depth': trial.suggest_int('max_depth', 5, 20),
        'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 1.0),
        'verbose': -1,
        'random_state': 42,
        'n_jobs': N_THREADS,
        'scale_pos_weight': (y_train_optuna == 0).sum() / (y_train_optuna == 1).sum()
    }
    
    train_data = lgb.Dataset(X_train_optuna, label=y_train_optuna)
    val_data = lgb.Dataset(X_val_proc, label=y_val, reference=train_data)
    
    model = lgb.train(
        params,
        train_data,
        valid_sets=[val_data],
        num_boost_round=1000,
        callbacks=[
            lgb.early_stopping(stopping_rounds=50, verbose=False),
            lgb.log_evaluation(0)
        ]
    )
    
    y_pred = model.predict(X_val_proc, num_iteration=model.best_iteration)
    score = roc_auc_score(y_val, y_pred)
    
    return score

study = optuna.create_study(
    direction='maximize',
    study_name='lgb_optimization_improved',
    sampler=optuna.samplers.TPESampler(seed=42)
)

study.optimize(objective, n_trials=50, show_progress_bar=True)

print("—Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:")
print(f"–õ—É—á—à–∏–π ROC-AUC: {study.best_value:.6f}")
print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
for key, value in study.best_params.items():
    print(f"  {key}: {value}")
print(f"{'='*60}\n")

print("\n–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –ø–æ–ª–Ω–æ–º –æ—á–∏—â–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ")

best_params = study.best_params.copy()
best_params['scale_pos_weight'] = (y_train_clean == 0).sum() / (y_train_clean == 1).sum()
best_params['objective'] = 'binary'
best_params['metric'] = 'auc'
best_params['boosting_type'] = 'gbdt'
best_params['verbose'] = -1
best_params['random_state'] = 42
best_params['n_jobs'] = N_THREADS

train_data_final = lgb.Dataset(X_train_proc, label=y_train_clean)
val_data_final = lgb.Dataset(X_val_proc, label=y_val, reference=train_data_final)

model_final = lgb.train(
    best_params,
    train_data_final,
    valid_sets=[val_data_final],
    num_boost_round=3000,
    callbacks=[
        lgb.early_stopping(stopping_rounds=150, verbose=True),
        lgb.log_evaluation(period=200)
    ]
)

val_pred_proba = model_final.predict(X_val_proc, num_iteration=model_final.best_iteration)
score_optuna = roc_auc_score(y_val, val_pred_proba)

print(f"ROC-AUC –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {score_optuna:.6f}")

joblib.dump(model_final, model_path)
print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")

print("\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
if 'score_v1' in globals():
    print(f"Pipeline v1 (RandomForest): {score_v1:.6f}")
if 'score_improved' in globals():
    print(f"–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ (6.2): {score_improved:.6f}")
print(f"–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å Optuna (6.3): {score_optuna:.6f}")
```

    –£–ª—É—á—à–µ–Ω–Ω—ã–π LightGBM —Å Optuna –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    
    [1/4] –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    –£–¥–∞–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: 20,574 (0.22%)
    
    [2/4] –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    
    [3/4] –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è Optuna
    –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: 8,883,264 —Å—Ç—Ä–æ–∫
    
    [4/4] –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å Optuna
    

    Best trial: 0. Best value: 0.879779:   2%|‚ñè         | 1/50 [01:10<57:20, 70.22s/it]

    [I 2025-12-18 14:47:28,935] Trial 0 finished with value: 0.8797786016991349 and parameters: {'num_leaves': 69, 'learning_rate': 0.24517932047070642, 'feature_fraction': 0.839196365086843, 'bagging_fraction': 0.759195090518222, 'bagging_freq': 2, 'min_child_samples': 35, 'reg_alpha': 3.3323645788192616e-08, 'reg_lambda': 0.6245760287469893, 'max_depth': 14, 'min_split_gain': 0.7080725777960455}. Best is trial 0 with value: 0.8797786016991349.
    

    Best trial: 0. Best value: 0.879779:   4%|‚ñç         | 2/50 [02:13<52:49, 66.02s/it]

    [I 2025-12-18 14:48:32,020] Trial 1 finished with value: 0.8786870243743922 and parameters: {'num_leaves': 22, 'learning_rate': 0.2652261985899886, 'feature_fraction': 0.899465584480253, 'bagging_fraction': 0.5274034664069657, 'bagging_freq': 2, 'min_child_samples': 40, 'reg_alpha': 5.472429642032198e-06, 'reg_lambda': 0.00052821153945323, 'max_depth': 11, 'min_split_gain': 0.2912291401980419}. Best is trial 0 with value: 0.8797786016991349.
    

    Best trial: 0. Best value: 0.879779:   6%|‚ñå         | 3/50 [05:50<1:45:44, 134.98s/it]

    [I 2025-12-18 14:52:09,066] Trial 2 finished with value: 0.8784428820706726 and parameters: {'num_leaves': 100, 'learning_rate': 0.008851384099881301, 'feature_fraction': 0.5752867891211308, 'bagging_fraction': 0.619817105976215, 'bagging_freq': 5, 'min_child_samples': 158, 'reg_alpha': 6.267062696005991e-07, 'reg_lambda': 0.00042472707398058225, 'max_depth': 14, 'min_split_gain': 0.046450412719997725}. Best is trial 0 with value: 0.8797786016991349.
    

    Best trial: 0. Best value: 0.879779:   8%|‚ñä         | 4/50 [10:09<2:20:58, 183.88s/it]

    [I 2025-12-18 14:56:27,906] Trial 3 finished with value: 0.8788180832001247 and parameters: {'num_leaves': 99, 'learning_rate': 0.0100505004670421, 'feature_fraction': 0.43903095579116774, 'bagging_fraction': 0.9693313223519999, 'bagging_freq': 10, 'min_child_samples': 163, 'reg_alpha': 5.514725787121931e-06, 'reg_lambda': 7.569183361880229e-08, 'max_depth': 15, 'min_split_gain': 0.4401524937396013}. Best is trial 0 with value: 0.8797786016991349.
    

    Best trial: 0. Best value: 0.879779:  10%|‚ñà         | 5/50 [13:35<2:23:53, 191.85s/it]

    [I 2025-12-18 14:59:53,883] Trial 4 finished with value: 0.8793878872283887 and parameters: {'num_leaves': 35, 'learning_rate': 0.03797252233376349, 'feature_fraction': 0.42063311266913106, 'bagging_fraction': 0.9455922412472693, 'bagging_freq': 3, 'min_child_samples': 134, 'reg_alpha': 6.388511557344611e-06, 'reg_lambda': 0.0004793052550782129, 'max_depth': 13, 'min_split_gain': 0.18485445552552704}. Best is trial 0 with value: 0.8797786016991349.
    

    Best trial: 5. Best value: 0.880237:  12%|‚ñà‚ñè        | 6/50 [16:41<2:19:21, 190.04s/it]

    [I 2025-12-18 15:03:00,414] Trial 5 finished with value: 0.8802371740634894 and parameters: {'num_leaves': 147, 'learning_rate': 0.11947399979512872, 'feature_fraction': 0.9636993649385135, 'bagging_fraction': 0.9368964102565893, 'bagging_freq': 6, 'min_child_samples': 185, 'reg_alpha': 6.257956190096665e-08, 'reg_lambda': 5.805581976088804e-07, 'max_depth': 5, 'min_split_gain': 0.32533033076326434}. Best is trial 5 with value: 0.8802371740634894.
    

    Best trial: 5. Best value: 0.880237:  14%|‚ñà‚ñç        | 7/50 [19:42<2:14:00, 186.98s/it]

    [I 2025-12-18 15:06:01,090] Trial 6 finished with value: 0.8761866650655099 and parameters: {'num_leaves': 70, 'learning_rate': 0.015186917176306207, 'feature_fraction': 0.8972425054911576, 'bagging_fraction': 0.6140519960161536, 'bagging_freq': 3, 'min_child_samples': 111, 'reg_alpha': 1.8548894229694903e-07, 'reg_lambda': 0.16587190283399655, 'max_depth': 6, 'min_split_gain': 0.9868869366005173}. Best is trial 5 with value: 0.8802371740634894.
    

    Best trial: 5. Best value: 0.880237:  16%|‚ñà‚ñå        | 8/50 [23:33<2:20:44, 201.05s/it]

    [I 2025-12-18 15:09:52,277] Trial 7 finished with value: 0.8788603168924877 and parameters: {'num_leaves': 121, 'learning_rate': 0.011280193301171913, 'feature_fraction': 0.4033132702741615, 'bagging_fraction': 0.8892768570729005, 'bagging_freq': 8, 'min_child_samples': 147, 'reg_alpha': 0.08738424135626986, 'reg_lambda': 4.638759594322625e-08, 'max_depth': 10, 'min_split_gain': 0.11586905952512971}. Best is trial 5 with value: 0.8802371740634894.
    

    Best trial: 5. Best value: 0.880237:  18%|‚ñà‚ñä        | 9/50 [26:19<2:09:55, 190.13s/it]

    [I 2025-12-18 15:12:38,394] Trial 8 finished with value: 0.8801114449844721 and parameters: {'num_leaves': 133, 'learning_rate': 0.06416354462323627, 'feature_fraction': 0.5985388149115896, 'bagging_fraction': 0.4381350101716142, 'bagging_freq': 4, 'min_child_samples': 68, 'reg_alpha': 0.036851536911881845, 'reg_lambda': 0.005470376807480391, 'max_depth': 19, 'min_split_gain': 0.4722149251619493}. Best is trial 5 with value: 0.8802371740634894.
    

    Best trial: 5. Best value: 0.880237:  20%|‚ñà‚ñà        | 10/50 [29:13<2:03:27, 185.19s/it]

    [I 2025-12-18 15:15:32,530] Trial 9 finished with value: 0.879828781326 and parameters: {'num_leaves': 35, 'learning_rate': 0.09273146363606355, 'feature_fraction': 0.8564710291701385, 'bagging_fraction': 0.7367663185416977, 'bagging_freq': 8, 'min_child_samples': 101, 'reg_alpha': 0.0005065186776865479, 'reg_lambda': 7.04480806377519e-05, 'max_depth': 5, 'min_split_gain': 0.10789142699330445}. Best is trial 5 with value: 0.8802371740634894.
    

    Best trial: 10. Best value: 0.880605:  22%|‚ñà‚ñà‚ñè       | 11/50 [32:14<1:59:31, 183.88s/it]

    [I 2025-12-18 15:18:33,424] Trial 10 finished with value: 0.8806047245849435 and parameters: {'num_leaves': 148, 'learning_rate': 0.11386711318218104, 'feature_fraction': 0.9817222664727194, 'bagging_fraction': 0.8300344207810204, 'bagging_freq': 7, 'min_child_samples': 182, 'reg_alpha': 4.3444691085504035, 'reg_lambda': 3.6789088151305197e-06, 'max_depth': 8, 'min_split_gain': 0.7032585954613222}. Best is trial 10 with value: 0.8806047245849435.
    

    Best trial: 10. Best value: 0.880605:  24%|‚ñà‚ñà‚ñç       | 12/50 [34:58<1:52:36, 177.81s/it]

    [I 2025-12-18 15:21:17,369] Trial 11 finished with value: 0.8805558996363476 and parameters: {'num_leaves': 150, 'learning_rate': 0.11976077456521955, 'feature_fraction': 0.9836380246874031, 'bagging_fraction': 0.8293855265853223, 'bagging_freq': 7, 'min_child_samples': 193, 'reg_alpha': 8.28996865017143, 'reg_lambda': 1.7912366914369137e-06, 'max_depth': 8, 'min_split_gain': 0.7093662388004657}. Best is trial 10 with value: 0.8806047245849435.
    

    Best trial: 10. Best value: 0.880605:  26%|‚ñà‚ñà‚ñå       | 13/50 [38:33<1:56:39, 189.18s/it]

    [I 2025-12-18 15:24:52,707] Trial 12 finished with value: 0.8801586798640159 and parameters: {'num_leaves': 147, 'learning_rate': 0.032091859295694795, 'feature_fraction': 0.9981860666460459, 'bagging_fraction': 0.8277359533133298, 'bagging_freq': 7, 'min_child_samples': 199, 'reg_alpha': 8.478767717461082, 'reg_lambda': 1.2167282577786403e-05, 'max_depth': 8, 'min_split_gain': 0.7481499229659709}. Best is trial 10 with value: 0.8806047245849435.
    

    Best trial: 13. Best value: 0.880647:  28%|‚ñà‚ñà‚ñä       | 14/50 [40:42<1:42:28, 170.79s/it]

    [I 2025-12-18 15:27:00,991] Trial 13 finished with value: 0.8806472176619891 and parameters: {'num_leaves': 121, 'learning_rate': 0.14882831012655023, 'feature_fraction': 0.7544846791129365, 'bagging_fraction': 0.8266129078957839, 'bagging_freq': 10, 'min_child_samples': 200, 'reg_alpha': 7.633177203131601, 'reg_lambda': 2.1770933269552202e-06, 'max_depth': 9, 'min_split_gain': 0.7014328475289627}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  30%|‚ñà‚ñà‚ñà       | 15/50 [42:32<1:29:02, 152.64s/it]

    [I 2025-12-18 15:28:51,586] Trial 14 finished with value: 0.8802952890584705 and parameters: {'num_leaves': 119, 'learning_rate': 0.18549567789369042, 'feature_fraction': 0.7513875993767701, 'bagging_fraction': 0.8145865702985663, 'bagging_freq': 10, 'min_child_samples': 173, 'reg_alpha': 0.15815334374680765, 'reg_lambda': 1.1249712509978433e-08, 'max_depth': 9, 'min_split_gain': 0.8774656636351008}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [45:31<1:30:52, 160.36s/it]

    [I 2025-12-18 15:31:49,863] Trial 15 finished with value: 0.8804042071566226 and parameters: {'num_leaves': 124, 'learning_rate': 0.0647443130712259, 'feature_fraction': 0.7655559737263076, 'bagging_fraction': 0.6423534961518614, 'bagging_freq': 9, 'min_child_samples': 130, 'reg_alpha': 0.6810212138640067, 'reg_lambda': 3.6478788617222495e-06, 'max_depth': 11, 'min_split_gain': 0.5800534667471442}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [49:02<1:36:37, 175.67s/it]

    [I 2025-12-18 15:35:21,133] Trial 16 finished with value: 0.8791548328106773 and parameters: {'num_leaves': 105, 'learning_rate': 0.024561813768650336, 'feature_fraction': 0.628518432257038, 'bagging_fraction': 0.8723409992340675, 'bagging_freq': 9, 'min_child_samples': 9, 'reg_alpha': 0.0028840533028087656, 'reg_lambda': 0.016109046194566698, 'max_depth': 7, 'min_split_gain': 0.5996569933793977}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [49:43<1:12:11, 135.35s/it]

    [I 2025-12-18 15:36:02,628] Trial 17 finished with value: 0.8783002098578618 and parameters: {'num_leaves': 134, 'learning_rate': 0.1619938846313904, 'feature_fraction': 0.6629553280122629, 'bagging_fraction': 0.6956056337681213, 'bagging_freq': 6, 'min_child_samples': 89, 'reg_alpha': 1.078109278682774, 'reg_lambda': 2.0616383701325948e-05, 'max_depth': 17, 'min_split_gain': 0.8426449988997252}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [52:49<1:17:41, 150.37s/it]

    [I 2025-12-18 15:39:07,998] Trial 18 finished with value: 0.8803807854300624 and parameters: {'num_leaves': 81, 'learning_rate': 0.058215173932824875, 'feature_fraction': 0.5165652428743162, 'bagging_fraction': 0.7735137752648358, 'bagging_freq': 8, 'min_child_samples': 180, 'reg_alpha': 0.0026984723914615628, 'reg_lambda': 4.777706609225013e-07, 'max_depth': 11, 'min_split_gain': 0.6116916050303964}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [55:08<1:13:34, 147.16s/it]

    [I 2025-12-18 15:41:27,658] Trial 19 finished with value: 0.8805698361846418 and parameters: {'num_leaves': 133, 'learning_rate': 0.10569879595344156, 'feature_fraction': 0.7748345251956631, 'bagging_fraction': 0.676926644210883, 'bagging_freq': 5, 'min_child_samples': 141, 'reg_alpha': 0.019180934401115693, 'reg_lambda': 6.789187154227627, 'max_depth': 9, 'min_split_gain': 0.8690193908638018}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [57:24<1:09:28, 143.75s/it]

    [I 2025-12-18 15:43:43,456] Trial 20 finished with value: 0.8805536398454317 and parameters: {'num_leaves': 109, 'learning_rate': 0.18132242130051626, 'feature_fraction': 0.7060174380851215, 'bagging_fraction': 0.8965377615472173, 'bagging_freq': 10, 'min_child_samples': 116, 'reg_alpha': 1.6077919879380673, 'reg_lambda': 1.9925761238260573e-07, 'max_depth': 7, 'min_split_gain': 0.9696048638251105}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [59:50<1:07:21, 144.36s/it]

    [I 2025-12-18 15:46:09,232] Trial 21 finished with value: 0.8803677883480832 and parameters: {'num_leaves': 135, 'learning_rate': 0.09211307979406452, 'feature_fraction': 0.7984768485689009, 'bagging_fraction': 0.5496602860968347, 'bagging_freq': 5, 'min_child_samples': 147, 'reg_alpha': 0.005514940386177238, 'reg_lambda': 0.012286332461979178, 'max_depth': 9, 'min_split_gain': 0.8341211348245541}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [1:01:57<1:02:39, 139.23s/it]

    [I 2025-12-18 15:48:16,502] Trial 22 finished with value: 0.88052594670309 and parameters: {'num_leaves': 138, 'learning_rate': 0.12180217261703476, 'feature_fraction': 0.7143890991368563, 'bagging_fraction': 0.7174336904232899, 'bagging_freq': 7, 'min_child_samples': 172, 'reg_alpha': 0.03729556126218253, 'reg_lambda': 9.879192474391058, 'max_depth': 9, 'min_split_gain': 0.7555052823087319}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [1:05:17<1:08:12, 157.39s/it]

    [I 2025-12-18 15:51:36,241] Trial 23 finished with value: 0.876601183515771 and parameters: {'num_leaves': 116, 'learning_rate': 0.005691124551370091, 'feature_fraction': 0.9279063601199924, 'bagging_fraction': 0.658946795830124, 'bagging_freq': 4, 'min_child_samples': 200, 'reg_alpha': 0.00015930095642062317, 'reg_lambda': 3.7932962316854446e-05, 'max_depth': 12, 'min_split_gain': 0.9029904355509184}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [1:07:59<1:06:06, 158.67s/it]

    [I 2025-12-18 15:54:17,919] Trial 24 finished with value: 0.8803782301751888 and parameters: {'num_leaves': 128, 'learning_rate': 0.05337755155925815, 'feature_fraction': 0.8030222732336209, 'bagging_fraction': 0.7899118698063129, 'bagging_freq': 1, 'min_child_samples': 154, 'reg_alpha': 0.3793030981874022, 'reg_lambda': 6.023221161217126, 'max_depth': 7, 'min_split_gain': 0.6406340150631071}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [1:11:05<1:06:45, 166.90s/it]

    [I 2025-12-18 15:57:24,011] Trial 25 finished with value: 0.880569240852972 and parameters: {'num_leaves': 89, 'learning_rate': 0.08398971587435759, 'feature_fraction': 0.8383702603859919, 'bagging_fraction': 0.8568827903561692, 'bagging_freq': 6, 'min_child_samples': 127, 'reg_alpha': 3.73754173751885, 'reg_lambda': 5.394052611916653e-06, 'max_depth': 10, 'min_split_gain': 0.7945187857948011}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [1:12:43<56:05, 146.32s/it]  

    [I 2025-12-18 15:59:02,328] Trial 26 finished with value: 0.8803575310006734 and parameters: {'num_leaves': 144, 'learning_rate': 0.14906171865556112, 'feature_fraction': 0.6637284850976587, 'bagging_fraction': 0.6924782273054517, 'bagging_freq': 9, 'min_child_samples': 181, 'reg_alpha': 0.013828682047041364, 'reg_lambda': 9.757346004270133e-05, 'max_depth': 8, 'min_split_gain': 0.5587792485027598}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [1:13:00<39:22, 107.38s/it]

    [I 2025-12-18 15:59:18,835] Trial 27 finished with value: 0.8780566983172182 and parameters: {'num_leaves': 113, 'learning_rate': 0.27852338103672925, 'feature_fraction': 0.514949891510385, 'bagging_fraction': 0.5891934881915195, 'bagging_freq': 4, 'min_child_samples': 170, 'reg_alpha': 0.00041674661299949395, 'reg_lambda': 0.0017843022151112632, 'max_depth': 12, 'min_split_gain': 0.6657584396595966}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [1:15:42<43:18, 123.74s/it]

    [I 2025-12-18 16:02:00,768] Trial 28 finished with value: 0.8780736284472394 and parameters: {'num_leaves': 127, 'learning_rate': 0.024708354420271115, 'feature_fraction': 0.7440633303747402, 'bagging_fraction': 0.48006256528574665, 'bagging_freq': 5, 'min_child_samples': 139, 'reg_alpha': 0.3378503425855785, 'reg_lambda': 0.13321869716896031, 'max_depth': 6, 'min_split_gain': 0.9300722757990632}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [1:16:23<32:59, 98.97s/it] 

    [I 2025-12-18 16:02:41,924] Trial 29 finished with value: 0.8792916479660482 and parameters: {'num_leaves': 91, 'learning_rate': 0.21489181343242128, 'feature_fraction': 0.8516991488550709, 'bagging_fraction': 0.7383023457298153, 'bagging_freq': 7, 'min_child_samples': 80, 'reg_alpha': 2.0744546461462545, 'reg_lambda': 1.0094101615272034e-06, 'max_depth': 10, 'min_split_gain': 0.5137271821307603}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [1:19:55<42:05, 132.93s/it]

    [I 2025-12-18 16:06:14,104] Trial 30 finished with value: 0.8804076448846389 and parameters: {'num_leaves': 139, 'learning_rate': 0.047190681531903846, 'feature_fraction': 0.9360910482793715, 'bagging_fraction': 0.9968452067147953, 'bagging_freq': 3, 'min_child_samples': 161, 'reg_alpha': 0.11161446817677995, 'reg_lambda': 8.425411535357546e-06, 'max_depth': 15, 'min_split_gain': 0.7719188969663449}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [1:22:55<44:07, 147.09s/it]

    [I 2025-12-18 16:09:14,243] Trial 31 finished with value: 0.8804274235578782 and parameters: {'num_leaves': 59, 'learning_rate': 0.08493275502598996, 'feature_fraction': 0.8058365949788795, 'bagging_fraction': 0.8659984142540703, 'bagging_freq': 6, 'min_child_samples': 121, 'reg_alpha': 3.4249916158675893, 'reg_lambda': 3.4294493217412053e-06, 'max_depth': 10, 'min_split_gain': 0.8008933820845732}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [1:26:00<44:55, 158.55s/it]

    [I 2025-12-18 16:12:19,523] Trial 32 finished with value: 0.8805613684734385 and parameters: {'num_leaves': 82, 'learning_rate': 0.0799227296494927, 'feature_fraction': 0.8909076300231001, 'bagging_fraction': 0.7815415341583778, 'bagging_freq': 6, 'min_child_samples': 188, 'reg_alpha': 4.709948328814978, 'reg_lambda': 1.8438038688238654, 'max_depth': 9, 'min_split_gain': 0.695886805437763}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [1:27:15<35:33, 133.37s/it]

    [I 2025-12-18 16:13:34,147] Trial 33 finished with value: 0.8798314527717215 and parameters: {'num_leaves': 63, 'learning_rate': 0.13620941007192228, 'feature_fraction': 0.8322786880205748, 'bagging_fraction': 0.8450064411053149, 'bagging_freq': 5, 'min_child_samples': 126, 'reg_alpha': 8.34556236657889e-05, 'reg_lambda': 0.00014585733550764044, 'max_depth': 10, 'min_split_gain': 0.8560712101867963}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [1:27:50<25:59, 103.95s/it]

    [I 2025-12-18 16:14:09,457] Trial 34 finished with value: 0.8788969656532164 and parameters: {'num_leaves': 98, 'learning_rate': 0.22897102473584885, 'feature_fraction': 0.768625555255091, 'bagging_fraction': 0.9131944978120189, 'bagging_freq': 8, 'min_child_samples': 146, 'reg_alpha': 0.8818741764967507, 'reg_lambda': 1.4318905576102537e-07, 'max_depth': 11, 'min_split_gain': 0.39588706317174743}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [1:28:36<20:10, 86.45s/it] 

    [I 2025-12-18 16:14:55,070] Trial 35 finished with value: 0.8791599144806428 and parameters: {'num_leaves': 91, 'learning_rate': 0.10836467208096885, 'feature_fraction': 0.8686285193334674, 'bagging_fraction': 0.8033992270641572, 'bagging_freq': 4, 'min_child_samples': 106, 'reg_alpha': 0.265240819110256, 'reg_lambda': 7.504484500470056e-06, 'max_depth': 14, 'min_split_gain': 0.716080280568984}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [1:31:34<24:40, 113.87s/it]

    [I 2025-12-18 16:17:52,906] Trial 36 finished with value: 0.8802962061426778 and parameters: {'num_leaves': 73, 'learning_rate': 0.08244504605721847, 'feature_fraction': 0.9335501966644649, 'bagging_fraction': 0.7538808214556554, 'bagging_freq': 5, 'min_child_samples': 160, 'reg_alpha': 2.4819959637871474, 'reg_lambda': 0.001316313389088021, 'max_depth': 13, 'min_split_gain': 0.8040833284674522}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [1:33:09<21:40, 108.37s/it]

    [I 2025-12-18 16:19:28,443] Trial 37 finished with value: 0.8798200089773704 and parameters: {'num_leaves': 45, 'learning_rate': 0.29851404844893376, 'feature_fraction': 0.7169887570634015, 'bagging_fraction': 0.9308948714548493, 'bagging_freq': 2, 'min_child_samples': 59, 'reg_alpha': 1.0305050066959612e-08, 'reg_lambda': 2.984133652449704e-08, 'max_depth': 8, 'min_split_gain': 0.9305485734178112}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [1:36:08<23:43, 129.41s/it]

    [I 2025-12-18 16:22:26,960] Trial 38 finished with value: 0.8801661205380875 and parameters: {'num_leaves': 141, 'learning_rate': 0.07153680191508902, 'feature_fraction': 0.668380124050848, 'bagging_fraction': 0.6729419705710111, 'bagging_freq': 6, 'min_child_samples': 96, 'reg_alpha': 7.506242265579771, 'reg_lambda': 3.511423654284729e-07, 'max_depth': 6, 'min_split_gain': 0.5186352895153151}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [1:39:44<25:53, 155.38s/it]

    [I 2025-12-18 16:26:02,926] Trial 39 finished with value: 0.8804225723417624 and parameters: {'num_leaves': 129, 'learning_rate': 0.04135029161268776, 'feature_fraction': 0.8267681396715115, 'bagging_fraction': 0.9719968396749633, 'bagging_freq': 7, 'min_child_samples': 169, 'reg_alpha': 1.557199192505099e-05, 'reg_lambda': 0.11784146916012503, 'max_depth': 12, 'min_split_gain': 0.6400709036641912}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [1:42:43<24:21, 162.42s/it]

    [I 2025-12-18 16:29:01,775] Trial 40 finished with value: 0.8802432177377161 and parameters: {'num_leaves': 106, 'learning_rate': 0.1028857300773461, 'feature_fraction': 0.9655555665390659, 'bagging_fraction': 0.587662613494618, 'bagging_freq': 9, 'min_child_samples': 139, 'reg_alpha': 0.03387555953143394, 'reg_lambda': 0.00023858000667982153, 'max_depth': 7, 'min_split_gain': 0.41886083697345716}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [1:45:40<22:16, 167.05s/it]

    [I 2025-12-18 16:31:59,631] Trial 41 finished with value: 0.8804907776172379 and parameters: {'num_leaves': 77, 'learning_rate': 0.07793861435714569, 'feature_fraction': 0.8904657673700934, 'bagging_fraction': 0.7821963354532104, 'bagging_freq': 6, 'min_child_samples': 191, 'reg_alpha': 3.9893993318105494, 'reg_lambda': 2.24475284842411, 'max_depth': 9, 'min_split_gain': 0.6874627387406246}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [1:48:08<18:49, 161.34s/it]

    [I 2025-12-18 16:34:27,632] Trial 42 finished with value: 0.8804823535110904 and parameters: {'num_leaves': 89, 'learning_rate': 0.13687436076497708, 'feature_fraction': 0.8853847580774489, 'bagging_fraction': 0.8573959810930851, 'bagging_freq': 6, 'min_child_samples': 188, 'reg_alpha': 1.1073766034736292, 'reg_lambda': 1.0539759154883142, 'max_depth': 10, 'min_split_gain': 0.7261979447370966}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 13. Best value: 0.880647:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [1:50:50<16:09, 161.55s/it]

    [I 2025-12-18 16:37:09,694] Trial 43 finished with value: 0.8804207979663626 and parameters: {'num_leaves': 48, 'learning_rate': 0.18454566936322553, 'feature_fraction': 0.9097022069455794, 'bagging_fraction': 0.7594772582630257, 'bagging_freq': 5, 'min_child_samples': 180, 'reg_alpha': 9.414361035633583, 'reg_lambda': 1.783085570662238, 'max_depth': 9, 'min_split_gain': 0.2440698599516669}. Best is trial 13 with value: 0.8806472176619891.
    

    Best trial: 44. Best value: 0.880691:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [1:54:01<14:11, 170.21s/it]

    [I 2025-12-18 16:40:20,117] Trial 44 finished with value: 0.8806908922507936 and parameters: {'num_leaves': 96, 'learning_rate': 0.10162578711558443, 'feature_fraction': 0.7798149581817775, 'bagging_fraction': 0.8366936126528723, 'bagging_freq': 8, 'min_child_samples': 200, 'reg_alpha': 9.352896985399261e-07, 'reg_lambda': 0.04355479172757652, 'max_depth': 8, 'min_split_gain': 0.8016739053601484}. Best is trial 44 with value: 0.8806908922507936.
    

    Best trial: 44. Best value: 0.880691:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [1:57:07<11:39, 174.84s/it]

    [I 2025-12-18 16:43:25,761] Trial 45 finished with value: 0.8800802358498416 and parameters: {'num_leaves': 98, 'learning_rate': 0.10873749106219098, 'feature_fraction': 0.7776112358626319, 'bagging_fraction': 0.894028546552829, 'bagging_freq': 8, 'min_child_samples': 199, 'reg_alpha': 1.1966812854284817e-06, 'reg_lambda': 0.32523743682342177, 'max_depth': 5, 'min_split_gain': 0.7945146861795237}. Best is trial 44 with value: 0.8806908922507936.
    

    Best trial: 44. Best value: 0.880691:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [2:00:17<08:58, 179.43s/it]

    [I 2025-12-18 16:46:35,908] Trial 46 finished with value: 0.8805204014288911 and parameters: {'num_leaves': 113, 'learning_rate': 0.15638989869388828, 'feature_fraction': 0.7412502958421091, 'bagging_fraction': 0.8144254974015885, 'bagging_freq': 7, 'min_child_samples': 177, 'reg_alpha': 2.0327327638797832e-07, 'reg_lambda': 0.03462106831422948, 'max_depth': 6, 'min_split_gain': 0.9825116494007826}. Best is trial 44 with value: 0.8806908922507936.
    

    Best trial: 44. Best value: 0.880691:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [2:02:55<05:46, 173.16s/it]

    [I 2025-12-18 16:49:14,442] Trial 47 finished with value: 0.8806528887837798 and parameters: {'num_leaves': 149, 'learning_rate': 0.12442536768649726, 'feature_fraction': 0.782829261287817, 'bagging_fraction': 0.8340920474682509, 'bagging_freq': 10, 'min_child_samples': 151, 'reg_alpha': 3.378433861800565e-05, 'reg_lambda': 1.5180870280156378e-06, 'max_depth': 8, 'min_split_gain': 0.8807737194209507}. Best is trial 44 with value: 0.8806908922507936.
    

    Best trial: 44. Best value: 0.880691:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [2:03:13<02:06, 126.61s/it]

    [I 2025-12-18 16:49:32,435] Trial 48 finished with value: 0.8783397054038087 and parameters: {'num_leaves': 149, 'learning_rate': 0.22529099163530672, 'feature_fraction': 0.6843414998550696, 'bagging_fraction': 0.7196047617918182, 'bagging_freq': 10, 'min_child_samples': 154, 'reg_alpha': 1.882266810079812e-05, 'reg_lambda': 1.23862646405971e-06, 'max_depth': 20, 'min_split_gain': 0.9024018973311455}. Best is trial 44 with value: 0.8806908922507936.
    

    Best trial: 44. Best value: 0.880691: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [2:05:55<00:00, 151.11s/it]
    

    [I 2025-12-18 16:52:14,316] Trial 49 finished with value: 0.8806206385296312 and parameters: {'num_leaves': 122, 'learning_rate': 0.1300651327277901, 'feature_fraction': 0.6335654026761135, 'bagging_fraction': 0.8270675787847714, 'bagging_freq': 10, 'min_child_samples': 164, 'reg_alpha': 5.077338711929052e-05, 'reg_lambda': 2.800909454542818e-05, 'max_depth': 8, 'min_split_gain': 0.8756661413740314}. Best is trial 44 with value: 0.8806908922507936.
    —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    –õ—É—á—à–∏–π ROC-AUC: 0.880691
    –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
      num_leaves: 96
      learning_rate: 0.10162578711558443
      feature_fraction: 0.7798149581817775
      bagging_fraction: 0.8366936126528723
      bagging_freq: 8
      min_child_samples: 200
      reg_alpha: 9.352896985399261e-07
      reg_lambda: 0.04355479172757652
      max_depth: 8
      min_split_gain: 0.8016739053601484
    ============================================================
    
    
    –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –ø–æ–ª–Ω–æ–º –æ—á–∏—â–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
    Training until validation scores don't improve for 150 rounds
    [200]	valid_0's auc: 0.879108
    [400]	valid_0's auc: 0.880043
    [600]	valid_0's auc: 0.880336
    [800]	valid_0's auc: 0.880475
    [1000]	valid_0's auc: 0.88052
    [1200]	valid_0's auc: 0.880548
    Early stopping, best iteration is:
    [1184]	valid_0's auc: 0.880549
    ROC-AUC –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: 0.880549
    –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: src\model_improved_optuna.pkl
    
    –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:
    Pipeline v1 (RandomForest): 0.860565
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ (6.2): 0.877647
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å Optuna (6.3): 0.880549
    

–û—Ç–ª–∏—á–Ω—ã–π –†–µ–∑—É–ª—å—Ç–∞—Ç `0.88055` –¥–∞–∂–µ —á—É—Ç—å –ª—É—á—à–µ —á–µ–º `0.87945` —É LightAutoML, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –∑–∞–Ω—è–ª–æ –≤–¥–≤–æ–µ –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏

### 6.4. –ü—Ä–æ—Å—Ç–æ–π CatBoost –ø–æ–¥—Ö–æ–¥

–ü—Ä–æ–±—É–µ–º CatBoost —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π, –ø–æ—Ç–æ–º—É —á—Ç–æ –≤ –∑–∞–¥–∞—á–∞—Ö —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –æ–Ω —á–∞—Å—Ç–æ —Å—Ä–∞–∑—É –¥–∞–µ—Ç —Å–∏–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –¢–∞–∫–æ–π —à–∞–≥ –Ω—É–∂–µ–Ω, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞—Å–∫–æ–ª—å–∫–æ CatBoost –≤—ã–∏–≥—Ä—ã–≤–∞–µ—Ç —Å–∞–º –ø–æ —Å–µ–±–µ, –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –ø–∞–π–ø–ª–∞–π–Ω–æ–≤.

#### –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è CatBoost


```python
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import StratifiedKFold

def simple_preprocess_catboost(X_train, X_val):
    X_train_proc = X_train.copy()
    X_val_proc = X_val.copy()
    
    if 'Annual_Premium' in X_train_proc.columns:
        X_train_proc['Annual_Premium'] = X_train_proc['Annual_Premium'].fillna(0).astype('int32')
        X_val_proc['Annual_Premium'] = X_val_proc['Annual_Premium'].fillna(0).astype('int32')
    
    num_cols = [col for col in X_train_proc.columns if X_train_proc[col].dtype in ['int64', 'float64']]
    num_cols = [col for col in num_cols if col != 'Annual_Premium']
    
    for col in num_cols:
        median_val = X_train_proc[col].median()
        if pd.isna(median_val):
            median_val = 0
        X_train_proc[col] = X_train_proc[col].fillna(median_val).astype('int16')
        X_val_proc[col] = X_val_proc[col].fillna(median_val).astype('int16')
    
    return X_train_proc, X_val_proc
```


```python
print("–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ CatBoost")

from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import StratifiedKFold

X_train_proc_simple, X_val_proc_simple = simple_preprocess_catboost(X_train, X_val)
cat_features = X_train_proc_simple.columns.values

sample_size = 1000000
if len(X_train_proc_simple) > sample_size:
    sample_idx = np.random.choice(len(X_train_proc_simple), sample_size, replace=False)
    X_train_sample = X_train_proc_simple.iloc[sample_idx]
    y_train_sample = y_train.iloc[sample_idx]
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ {sample_size:,} —Å—Ç—Ä–æ–∫")
else:
    X_train_sample = X_train_proc_simple
    y_train_sample = y_train

cat_params = {
    'loss_function': 'Logloss',
    'eval_metric': 'AUC',
    'learning_rate': 0.03,
    'iterations': 3000,
    'depth': 6,
    'l2_leaf_reg': 3.0,
    'random_strength': 1.0,
    'bagging_temperature': 1.0,
    'border_count': 254,
    'colsample_bylevel': 1.0,
    'min_data_in_leaf': 1,
    'random_seed': 42,
    'task_type': 'GPU',
    'gpu_ram_part': 0.9,
    'verbose': False
}

n_folds = 5
skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
models_simple = []
oof_predictions = np.zeros(len(X_train_sample))

print(f"–û–±—É—á–µ–Ω–∏–µ {n_folds} —Ñ–æ–ª–¥–æ–≤")
for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_sample, y_train_sample)):
    print(f"  Fold {fold+1}/{n_folds}")
    X_tr, X_te = X_train_sample.iloc[train_idx], X_train_sample.iloc[val_idx]
    y_tr, y_te = y_train_sample.iloc[train_idx], y_train_sample.iloc[val_idx]
    
    X_tr_pool = Pool(X_tr, y_tr, cat_features=cat_features)
    X_te_pool = Pool(X_te, y_te, cat_features=cat_features)
    
    model = CatBoostClassifier(**cat_params)
    model.fit(X_tr_pool, eval_set=X_te_pool, verbose=1000, early_stopping_rounds=200)
    
    oof_predictions[val_idx] = model.predict_proba(X_te_pool)[:, 1]
    models_simple.append(model)

cv_score_simple = roc_auc_score(y_train_sample, oof_predictions)
print(f"CV ROC-AUC: {cv_score_simple:.6f}")

X_val_pool = Pool(X_val_proc_simple, cat_features=cat_features)
val_predictions_simple = np.mean([m.predict_proba(X_val_pool)[:, 1] for m in models_simple], axis=0)
score_simple = roc_auc_score(y_val, val_predictions_simple)
print(f"ROC-AUC –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {score_simple:.6f}")

joblib.dump({'models': models_simple, 'cat_features': cat_features}, MODELS_DIR / 'catboost_simple_final.pkl')
print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODELS_DIR / 'catboost_simple_final.pkl'}")
```

    –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ CatBoost
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ 1,000,000 —Å—Ç—Ä–æ–∫
    –û–±—É—á–µ–Ω–∏–µ 5 —Ñ–æ–ª–¥–æ–≤
      Fold 1/5
    

    Default metric period is 5 because AUC is/are not implemented for GPU
    

    0:	test: 0.8616198	best: 0.8616198 (0)	total: 54.1ms	remaining: 2m 42s
    1000:	test: 0.8860235	best: 0.8860244 (998)	total: 25.3s	remaining: 50.6s
    2000:	test: 0.8865267	best: 0.8865294 (1989)	total: 48s	remaining: 24s
    2999:	test: 0.8866575	best: 0.8866630 (2977)	total: 1m 10s	remaining: 0us
    bestTest = 0.8866629601
    bestIteration = 2977
    Shrink model to first 2978 iterations.
      Fold 2/5
    

    Default metric period is 5 because AUC is/are not implemented for GPU
    

    0:	test: 0.8614763	best: 0.8614763 (0)	total: 34.8ms	remaining: 1m 44s
    1000:	test: 0.8864266	best: 0.8864266 (1000)	total: 23.9s	remaining: 47.8s
    2000:	test: 0.8868688	best: 0.8868691 (1993)	total: 47.1s	remaining: 23.5s
    2999:	test: 0.8870326	best: 0.8870361 (2963)	total: 1m 10s	remaining: 0us
    bestTest = 0.8870360851
    bestIteration = 2963
    Shrink model to first 2964 iterations.
      Fold 3/5
    

    Default metric period is 5 because AUC is/are not implemented for GPU
    

    0:	test: 0.8607248	best: 0.8607248 (0)	total: 26.8ms	remaining: 1m 20s
    1000:	test: 0.8865068	best: 0.8865068 (1000)	total: 23.7s	remaining: 47.3s
    2000:	test: 0.8869871	best: 0.8869874 (1999)	total: 47s	remaining: 23.5s
    2999:	test: 0.8871457	best: 0.8871462 (2997)	total: 1m 9s	remaining: 0us
    bestTest = 0.8871462345
    bestIteration = 2997
    Shrink model to first 2998 iterations.
      Fold 4/5
    

    Default metric period is 5 because AUC is/are not implemented for GPU
    

    0:	test: 0.8609999	best: 0.8609999 (0)	total: 37.5ms	remaining: 1m 52s
    1000:	test: 0.8863213	best: 0.8863213 (1000)	total: 24.2s	remaining: 48.3s
    2000:	test: 0.8869185	best: 0.8869185 (2000)	total: 47.7s	remaining: 23.8s
    2999:	test: 0.8871346	best: 0.8871393 (2981)	total: 1m 10s	remaining: 0us
    bestTest = 0.8871392608
    bestIteration = 2981
    Shrink model to first 2982 iterations.
      Fold 5/5
    

    Default metric period is 5 because AUC is/are not implemented for GPU
    

    0:	test: 0.8619809	best: 0.8619809 (0)	total: 37.2ms	remaining: 1m 51s
    1000:	test: 0.8859610	best: 0.8859610 (1000)	total: 23.3s	remaining: 46.5s
    2000:	test: 0.8863873	best: 0.8863873 (2000)	total: 46.2s	remaining: 23s
    2999:	test: 0.8865442	best: 0.8865442 (2999)	total: 1m 8s	remaining: 0us
    bestTest = 0.8865442276
    bestIteration = 2999
    CV ROC-AUC: 0.886898
    ROC-AUC –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: 0.887494
    –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: src\models\catboost_simple_final.pkl
    

–ü—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–æ –±–æ–ª–µ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç `0.88749` —É cb –±–ª–∞–≥–æ–¥–∞—Ä—è —Ö–æ—Ä–æ—à–µ–π —Ä–∞–±–æ—Ç–µ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏, –Ω–∞–º–Ω–æ–≥–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç lgbm `0.88055` –∏ `0.87945` —É LightAutoML, –±—É–¥–µ–º –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ä–∞–±–æ—Ç—É –Ω–∞–¥ –∫–∞—Ç–±—É—Å—Ç–æ–º. –ü—Ä–∏ —ç—Ç–æ–º –Ω–µ–º–Ω–æ–≥–æ –±—ã—Å—Ç—Ä–µ–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —á–µ–º –¥—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã

### 6.5. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π CatBoost —Å feature engineering

–ù–∞ —ç—Ç–æ–º —à–∞–≥–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —É—Å–ª–æ–∂–Ω—è–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–∏–ø–æ—Ç–µ–∑—É –∏–∑ EDA, —á—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–æ–≥—É—Ç –ø–æ–¥–Ω—è—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ. –ß–∏—Å—Ç–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —á–∞—Å—Ç—å –≤—ã–±—Ä–æ—Å–æ–≤, –¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Å–Ω–æ–≤–∞ –æ–±—É—á–∞–µ–º CatBoost.


```python
model_path = MODELS_DIR / 'catboost_advanced_final.pkl'

def remove_duplicates_advanced(X, y=None):
    initial_size = len(X)
    if y is not None:
        X_with_target = X.copy()
        X_with_target['_target'] = y.values
        X_with_target = X_with_target.drop_duplicates()
        y_cleaned = pd.Series(X_with_target['_target'].values, index=X_with_target.index)
        X_cleaned = X_with_target.drop('_target', axis=1)
        duplicates_removed = initial_size - len(X_cleaned)
        if duplicates_removed > 0:
            print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed:,} ({duplicates_removed/initial_size*100:.2f}%)")
        return X_cleaned, y_cleaned
    else:
        X_cleaned = X.drop_duplicates()
        duplicates_removed = initial_size - len(X_cleaned)
        if duplicates_removed > 0:
            print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed:,} ({duplicates_removed/initial_size*100:.2f}%)")
        return X_cleaned

def detect_and_remove_anomalies_advanced(X, y=None, z_threshold=3.5, iqr_factor=2.0):
    initial_size = len(X)
    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
    anomaly_indices = set()
    
    for col in numeric_cols:
        col_data = X[col].dropna()
        if len(col_data) == 0:
            continue
        z_scores = np.abs(stats.zscore(col_data))
        z_outliers = col_data.index[z_scores > z_threshold].tolist()
        Q1 = col_data.quantile(0.25)
        Q3 = col_data.quantile(0.75)
        IQR = Q3 - Q1
        if IQR > 0:
            lower_bound = Q1 - iqr_factor * IQR
            upper_bound = Q3 + iqr_factor * IQR
            iqr_outliers = col_data.index[(col_data < lower_bound) | (col_data > upper_bound)].tolist()
        else:
            iqr_outliers = []
        outliers = set(z_outliers) | set(iqr_outliers)
        anomaly_indices.update(outliers)
    
    if len(anomaly_indices) > 0:
        anomaly_pct = len(anomaly_indices) / initial_size * 100
        if anomaly_pct < 8.0:
            X_cleaned = X.drop(index=anomaly_indices)
            if y is not None:
                y_cleaned = y.drop(index=anomaly_indices)
                print(f"–£–¥–∞–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {len(anomaly_indices):,} ({anomaly_pct:.2f}%)")
                return X_cleaned, y_cleaned
            else:
                print(f"–£–¥–∞–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {len(anomaly_indices):,} ({anomaly_pct:.2f}%)")
                return X_cleaned
    
    if y is not None:
        return X, y
    else:
        return X

def create_features_advanced(df, is_train=False):
    df = df.copy()
    if 'Annual_Premium' in df.columns:
        df['Annual_Premium_log'] = np.log1p(df['Annual_Premium'])
        df['Annual_Premium_sqrt'] = np.sqrt(df['Annual_Premium'])
    if 'Age' in df.columns:
        df['Age_group'] = pd.cut(df['Age'], bins=[0, 25, 35, 45, 55, 100], labels=['<25', '25-35', '35-45', '45-55', '55+'])
        df['Age_group'] = df['Age_group'].astype(str)
        df['Age_squared'] = df['Age'] ** 2
        df['Age_cubed'] = df['Age'] ** 3
    if 'Vintage' in df.columns:
        df['Vintage_log'] = np.log1p(df['Vintage'])
    if 'Age' in df.columns and 'Annual_Premium' in df.columns:
        df['Age_Premium_interaction'] = df['Age'] * df['Annual_Premium'] / 1000
        df['Age_Premium_ratio'] = df['Annual_Premium'] / (df['Age'] + 1)
    if 'Vehicle_Age' in df.columns and 'Vehicle_Damage' in df.columns:
        df['Vehicle_combination'] = df['Vehicle_Age'].astype(str) + '_' + df['Vehicle_Damage'].astype(str)
    if 'Previously_Insured' in df.columns and 'Vehicle_Damage' in df.columns:
        df['Insured_Damage'] = df['Previously_Insured'].astype(str) + '_' + df['Vehicle_Damage'].astype(str)
    if 'Region_Code' in df.columns and 'Policy_Sales_Channel' in df.columns:
        df['Region_Channel'] = df['Region_Code'].astype(str) + '_' + df['Policy_Sales_Channel'].astype(str)
    return df

def preprocess_data_advanced(X_train, X_val):
    X_train_proc = create_features_advanced(X_train, is_train=True)
    X_val_proc = create_features_advanced(X_val, is_train=False)
    
    if 'Annual_Premium' in X_train_proc.columns:
        X_train_proc['Annual_Premium'] = X_train_proc['Annual_Premium'].fillna(0).astype('int32')
        X_val_proc['Annual_Premium'] = X_val_proc['Annual_Premium'].fillna(0).astype('int32')
    
    num_cols = [col for col in X_train_proc.columns if X_train_proc[col].dtype in ['int64', 'float64']]
    num_cols = [col for col in num_cols if col != 'Annual_Premium']
    
    for col in num_cols:
        median_val = X_train_proc[col].median()
        if pd.isna(median_val):
            median_val = 0
        X_train_proc[col] = X_train_proc[col].fillna(median_val).astype('int16')
        X_val_proc[col] = X_val_proc[col].fillna(median_val).astype('int16')
    
    cat_features = [col for col in X_train_proc.columns if X_train_proc[col].dtype == 'object']
    
    return X_train_proc, X_val_proc, cat_features

print("–û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ CatBoost")

X_train_clean, y_train_clean = remove_duplicates_advanced(X_train.copy(), y_train.copy())
X_train_clean, y_train_clean = detect_and_remove_anomalies_advanced(X_train_clean, y_train_clean)
X_train_proc_adv, X_val_proc_adv, cat_features_adv = preprocess_data_advanced(X_train_clean, X_val.copy())

sample_size = 1000000
if len(X_train_proc_adv) > sample_size:
    sample_idx = np.random.choice(len(X_train_proc_adv), sample_size, replace=False)
    X_train_sample = X_train_proc_adv.iloc[sample_idx]
    y_train_sample = y_train_clean.iloc[sample_idx]
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ {sample_size:,} —Å—Ç—Ä–æ–∫")
else:
    X_train_sample = X_train_proc_adv
    y_train_sample = y_train_clean

cat_params = {
    'loss_function': 'Logloss',
    'eval_metric': 'AUC',
    'learning_rate': 0.03,
    'iterations': 3000,
    'depth': 6,
    'l2_leaf_reg': 3.0,
    'random_strength': 1.0,
    'bagging_temperature': 1.0,
    'border_count': 254,
    'colsample_bylevel': 1.0,
    'min_data_in_leaf': 1,
    'random_seed': 42,
    'verbose': False
}

n_folds = 5
skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
models_advanced = []
oof_predictions = np.zeros(len(X_train_sample))

print(f"–û–±—É—á–µ–Ω–∏–µ {n_folds} —Ñ–æ–ª–¥–æ–≤")
for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_sample, y_train_sample)):
    print(f"  Fold {fold+1}/{n_folds}")
    X_tr, X_te = X_train_sample.iloc[train_idx], X_train_sample.iloc[val_idx]
    y_tr, y_te = y_train_sample.iloc[train_idx], y_train_sample.iloc[val_idx]
    
    X_tr_pool = Pool(X_tr, y_tr, cat_features=cat_features_adv)
    X_te_pool = Pool(X_te, y_te, cat_features=cat_features_adv)
    
    model = CatBoostClassifier(**cat_params)
    model.fit(X_tr_pool, eval_set=X_te_pool, verbose=1000, early_stopping_rounds=200)
    
    oof_predictions[val_idx] = model.predict_proba(X_te_pool)[:, 1]
    models_advanced.append(model)

cv_score_advanced = roc_auc_score(y_train_sample, oof_predictions)
print(f"CV ROC-AUC: {cv_score_advanced:.6f}")

X_val_pool_adv = Pool(X_val_proc_adv, cat_features=cat_features_adv)
val_predictions_advanced = np.mean([m.predict_proba(X_val_pool_adv)[:, 1] for m in models_advanced], axis=0)
score_advanced = roc_auc_score(y_val, val_predictions_advanced)
print(f"ROC-AUC –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {score_advanced:.6f}")

joblib.dump({'models': models_advanced, 'cat_features': cat_features_adv}, MODELS_DIR / 'catboost_advanced_final.pkl')
print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {MODELS_DIR / 'catboost_advanced_final.pkl'}")
```

    –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ CatBoost
    –£–¥–∞–ª–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: 95,939 (1.04%)
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ 800,000 —Å—Ç—Ä–æ–∫
    –û–±—É—á–µ–Ω–∏–µ 5 —Ñ–æ–ª–¥–æ–≤
      Fold 1/5
    0:	test: 0.8279962	best: 0.8279962 (0)	total: 218ms	remaining: 10m 53s
    1000:	test: 0.8720287	best: 0.8720287 (1000)	total: 1m 49s	remaining: 3m 38s
    2000:	test: 0.8741949	best: 0.8741949 (2000)	total: 3m 45s	remaining: 1m 52s
    2999:	test: 0.8749932	best: 0.8749932 (2999)	total: 5m 43s	remaining: 0us
    
    bestTest = 0.8749931853
    bestIteration = 2999
    
      Fold 2/5
    0:	test: 0.8430991	best: 0.8430991 (0)	total: 127ms	remaining: 6m 22s
    1000:	test: 0.8766117	best: 0.8766123 (996)	total: 1m 51s	remaining: 3m 43s
    2000:	test: 0.8785238	best: 0.8785239 (1998)	total: 3m 48s	remaining: 1m 54s
    2999:	test: 0.8792830	best: 0.8792830 (2999)	total: 5m 46s	remaining: 0us
    
    bestTest = 0.8792830024
    bestIteration = 2999
    
      Fold 3/5
    0:	test: 0.8419311	best: 0.8419311 (0)	total: 135ms	remaining: 6m 45s
    1000:	test: 0.8753691	best: 0.8753693 (999)	total: 1m 52s	remaining: 3m 45s
    2000:	test: 0.8776789	best: 0.8776789 (2000)	total: 3m 50s	remaining: 1m 54s
    2999:	test: 0.8785685	best: 0.8785685 (2999)	total: 5m 47s	remaining: 0us
    
    bestTest = 0.8785685141
    bestIteration = 2999
    
      Fold 4/5
    0:	test: 0.8389050	best: 0.8389050 (0)	total: 132ms	remaining: 6m 34s
    1000:	test: 0.8736826	best: 0.8736826 (1000)	total: 1m 48s	remaining: 3m 37s
    2000:	test: 0.8760144	best: 0.8760144 (2000)	total: 3m 41s	remaining: 1m 50s
    2999:	test: 0.8768385	best: 0.8768385 (2999)	total: 5m 34s	remaining: 0us
    
    bestTest = 0.8768384712
    bestIteration = 2999
    
      Fold 5/5
    0:	test: 0.8405894	best: 0.8405894 (0)	total: 142ms	remaining: 7m 6s
    1000:	test: 0.8726445	best: 0.8726445 (1000)	total: 1m 48s	remaining: 3m 37s
    2000:	test: 0.8749744	best: 0.8749744 (2000)	total: 3m 42s	remaining: 1m 51s
    2999:	test: 0.8758036	best: 0.8758054 (2991)	total: 5m 41s	remaining: 0us
    
    bestTest = 0.8758053862
    bestIteration = 2991
    
    Shrink model to first 2992 iterations.
    CV ROC-AUC: 0.877092
    ROC-AUC –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: 0.876462
    –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: src\catboost_advanced_final.pkl
    

 –ò–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ `0.87646` –ø—Ä–æ—Ç–∏–≤ `0.88749` –≤–∏–¥–Ω–æ —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Ö—É–¥—à–∏–ª–∏—Å—å, –∫–∞—Ç–±—É—Å—Ç—É –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–º–µ—é—â–∏—Ö—Å—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏, –ø–æ—ç—Ç–æ–º—É –¥–∞–ª—å—à–µ —Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞–¥ –ø—Ä–æ—Å—Ç—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º

### 6.6. CatBoost —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ Optuna

–ó–¥–µ—Å—å –æ—Å—Ç–∞–≤–ª—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –∫–∞–∫ –≤ –ø—Ä–æ—Å—Ç–æ–º CatBoost, –Ω–æ –ø–æ–¥–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ Optuna. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–µ–ª–∞–µ–º –Ω–∞ –º–µ–Ω—å—à–µ–π –≤—ã–±–æ—Ä–∫–µ, —á—Ç–æ–±—ã —ç—Ç–æ –∑–∞–Ω–∏–º–∞–ª–æ —Ä–∞–∑—É–º–Ω–æ–µ –≤—Ä–µ–º—è, –∞ –ø–æ—Ç–æ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ò–¥–µ—è –ø—Ä–æ—Å—Ç–∞—è, –µ—Å–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ –µ—â–µ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å, —Ç–æ —á–∞—â–µ –≤—Å–µ–≥–æ —ç—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –º–æ–¥–µ–ª–∏, –∞ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –Ω–æ–≤—ã—Ö —Ñ–∏—á.


```python
sample_size = 6000000
used_ram_limit = '24GB'

import optuna
from optuna.samplers import TPESampler
import json
import numpy as np
from pathlib import Path
import joblib
import gc

print("–û–±—É—á–µ–Ω–∏–µ CatBoost —Å Optuna")

X_train_proc_optuna, X_val_proc_optuna = simple_preprocess_catboost(X_train, X_val)
cat_features_optuna = X_train_proc_optuna.columns.values

params_file = PARAMS_DIR / 'catboost_optuna_params.json'
best_params = None
best_auc_optuna = None

if params_file.exists():
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ {params_file}")
    with open(params_file, 'r') as f:
        saved_data = json.load(f)
        best_params = saved_data.get('best_params')
        best_auc_optuna = saved_data.get('best_auc')
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω AUC: {best_auc_optuna:.6f}")
else:
    print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∑–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é")
    
    optuna_sample_size = min(500000, len(X_train_proc_optuna))
    optuna_idx = np.random.choice(len(X_train_proc_optuna), optuna_sample_size, replace=False)
    X_optuna = X_train_proc_optuna.iloc[optuna_idx]
    y_optuna = y_train.iloc[optuna_idx]
    print(f"–î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã–±–æ—Ä–∫–∞ {optuna_sample_size:,} —Å—Ç—Ä–æ–∫")

    def objective(trial):
        bootstrap_type = trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli'])
        
        params = {
            'loss_function': 'Logloss',
            'eval_metric': 'AUC',
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),
            'iterations': trial.suggest_int('iterations', 2000, 5000),
            'depth': trial.suggest_int('depth', 4, 10),
            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0, log=True),
            'random_strength': trial.suggest_float('random_strength', 0.0, 10.0),
            'border_count': trial.suggest_int('border_count', 32, 255),
            'bootstrap_type': bootstrap_type,
            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 20),
            'task_type': 'GPU',
            'random_seed': 42,
            'verbose': False
        }
        
        params['colsample_bylevel'] = trial.suggest_float('colsample_bylevel', 0.6, 1.0)
        
        if bootstrap_type == 'Bayesian':
            params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0.0, 1.0)
        else:
            params['subsample'] = trial.suggest_float('subsample', 0.6, 1.0)
        
        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
        scores = []
        
        for train_idx, val_idx in skf.split(X_optuna, y_optuna):
            X_tr, X_v = X_optuna.iloc[train_idx], X_optuna.iloc[val_idx]
            y_tr, y_v = y_optuna.iloc[train_idx], y_optuna.iloc[val_idx]
            
            X_tr_pool = Pool(X_tr, y_tr, cat_features=cat_features_optuna)
            X_v_pool = Pool(X_v, y_v, cat_features=cat_features_optuna)
            
            model = CatBoostClassifier(**params)
            model.fit(X_tr_pool, eval_set=X_v_pool, verbose=False, early_stopping_rounds=200)
            
            y_pred = model.predict_proba(X_v_pool)[:, 1]
            scores.append(roc_auc_score(y_v, y_pred))
        
        return np.mean(scores)

    print("–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))
    study.optimize(objective, n_trials=50, show_progress_bar=True)

    best_params = study.best_params
    best_auc_optuna = study.best_value
    print(f"–õ—É—á—à–∏–π AUC: {best_auc_optuna:.6f}")
    print(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {best_params}")
    
    params_data = {
        'best_params': best_params,
        'best_auc': best_auc_optuna
    }
    with open(params_file, 'w') as f:
        json.dump(params_data, f, indent=2)
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

cat_params = {
    'loss_function': 'Logloss',
    'eval_metric': 'AUC',
    'random_seed': 42,
    'verbose': False,
    'task_type': 'GPU',
    'gpu_ram_part': 0.95,
    'used_ram_limit': used_ram_limit,
    **best_params
}

folds_dir = MODELS_DIR / 'catboost_folds'
folds_dir.mkdir(parents=True, exist_ok=True)

sample_idx_path = folds_dir / 'sample_idx.npy'

if sample_idx_path.exists():
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤—ã–±–æ—Ä–∫–∏ –∏–∑ {sample_idx_path}")
    sample_idx = np.load(sample_idx_path)
    X_train_sample = X_train_proc_optuna.iloc[sample_idx]
    y_train_sample = y_train.iloc[sample_idx]
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ {len(X_train_sample):,} —Å—Ç—Ä–æ–∫")
else:
    if len(X_train_proc_optuna) > sample_size:
        sample_idx = np.random.choice(len(X_train_proc_optuna), sample_size, replace=False)
        X_train_sample = X_train_proc_optuna.iloc[sample_idx]
        y_train_sample = y_train.iloc[sample_idx]
        np.save(sample_idx_path, sample_idx)
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ {sample_size:,} —Å—Ç—Ä–æ–∫")
    else:
        X_train_sample = X_train_proc_optuna
        y_train_sample = y_train
        sample_idx = np.arange(len(X_train_sample))
        np.save(sample_idx_path, sample_idx)
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–ª–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ {len(X_train_sample):,} —Å—Ç—Ä–æ–∫")

n_folds = 5
skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)
oof_predictions = np.zeros(len(X_train_sample))

trained_folds_path = folds_dir / 'trained_folds.npy'
oof_path = folds_dir / 'oof_predictions.npy'

trained_folds = []
if trained_folds_path.exists():
    trained_folds = np.load(trained_folds_path).tolist()
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(trained_folds)} —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö —Ñ–æ–ª–¥–æ–≤: {trained_folds}")
    if oof_path.exists():
        oof_predictions = np.load(oof_path)

print(f"–û–±—É—á–µ–Ω–∏–µ {n_folds} —Ñ–æ–ª–¥–æ–≤")
for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_sample, y_train_sample)):
    if fold in trained_folds:
        print(f"Fold {fold+1}/{n_folds} –ø—Ä–æ–ø—É—â–µ–Ω (—É–∂–µ –æ–±—É—á–µ–Ω)")
        continue

    print(f"Fold {fold+1}/{n_folds}")
    X_tr, X_te = X_train_sample.iloc[train_idx], X_train_sample.iloc[val_idx]
    y_tr, y_te = y_train_sample.iloc[train_idx], y_train_sample.iloc[val_idx]
    
    X_tr_pool = Pool(X_tr, y_tr, cat_features=cat_features_optuna)
    X_te_pool = Pool(X_te, y_te, cat_features=cat_features_optuna)
    
    model = CatBoostClassifier(**cat_params)
    model.fit(X_tr_pool, eval_set=X_te_pool, verbose=1000, early_stopping_rounds=200)
    
    model_path = folds_dir / f'catboost_fold_{fold}.cbm'
    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")
    model.save_model(str(model_path))
    print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
    
    print(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    oof_predictions[val_idx] = model.predict_proba(X_te_pool)[:, 1]
    
    del model
    del X_tr_pool, X_te_pool
    del X_tr, X_te, y_tr, y_te
    gc.collect()
    
    trained_folds.append(fold)
    np.save(oof_path, oof_predictions)
    np.save(trained_folds_path, np.array(trained_folds, dtype=int))
    print(f"  Fold {fold+1}/{n_folds} –∑–∞–≤–µ—Ä—à–µ–Ω")

cv_score_optuna = roc_auc_score(y_train_sample, oof_predictions)
print(f"CV ROC-AUC: {cv_score_optuna:.6f}")

print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
models_optuna = []
for fold in trained_folds:
    model_path_cbm = folds_dir / f'catboost_fold_{fold}.cbm'
    if model_path_cbm.exists():
        model = CatBoostClassifier()
        model.load_model(str(model_path_cbm))
        models_optuna.append(model)

X_val_pool_optuna = Pool(X_val_proc_optuna, cat_features=cat_features_optuna)
val_predictions_optuna = np.mean([m.predict_proba(X_val_pool_optuna)[:, 1] for m in models_optuna], axis=0)
score_optuna = roc_auc_score(y_val, val_predictions_optuna)
print(f"ROC-AUC –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {score_optuna:.6f}")

del models_optuna, X_val_pool_optuna, val_predictions_optuna
gc.collect()

joblib.dump(
    {
        'model_paths': [str(folds_dir / f'catboost_fold_{i}.cbm') for i in trained_folds],
        'cat_features': cat_features_optuna,
        'best_params': best_params
    },
    MODELS_DIR / 'catboost_optuna_models.pkl'
)
with open(PARAMS_DIR / 'catboost_optuna_params.json', 'w') as f:
    json.dump({'best_params': best_params, 'best_auc': best_auc_optuna, 'cv_auc': cv_score_optuna}, f, indent=2)
print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
```

    –û–±—É—á–µ–Ω–∏–µ CatBoost —Å Optuna
    –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ src\params\catboost_optuna_params.json
    –ó–∞–≥—Ä—É–∂–µ–Ω AUC: 0.882380
    –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤—ã–±–æ—Ä–∫–∏ –∏–∑ src\models\catboost_folds\sample_idx.npy
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ 6,000,000 —Å—Ç—Ä–æ–∫
    –ù–∞–π–¥–µ–Ω–æ 4 —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö —Ñ–æ–ª–¥–æ–≤: [0, 1, 2, 3]
    –û–±—É—á–µ–Ω–∏–µ 5 —Ñ–æ–ª–¥–æ–≤
    Fold 1/5 –ø—Ä–æ–ø—É—â–µ–Ω (—É–∂–µ –æ–±—É—á–µ–Ω)
    Fold 2/5 –ø—Ä–æ–ø—É—â–µ–Ω (—É–∂–µ –æ–±—É—á–µ–Ω)
    Fold 3/5 –ø—Ä–æ–ø—É—â–µ–Ω (—É–∂–µ –æ–±—É—á–µ–Ω)
    Fold 4/5 –ø—Ä–æ–ø—É—â–µ–Ω (—É–∂–µ –æ–±—É—á–µ–Ω)
    Fold 5/5
    

    Default metric period is 5 because AUC is/are not implemented for GPU
    

    0:	test: 0.8643126	best: 0.8643126 (0)	total: 427ms	remaining: 17m 32s
    1000:	test: 0.8924683	best: 0.8924683 (1000)	total: 3m 18s	remaining: 4m 50s
    2000:	test: 0.8929046	best: 0.8929046 (1998)	total: 5m 49s	remaining: 1m 21s
    2465:	test: 0.8929967	best: 0.8929967 (2465)	total: 6m 58s	remaining: 0us
    bestTest = 0.8929966688
    bestIteration = 2465
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ src\models\catboost_folds\catboost_fold_4.cbm
    –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
      Fold 5/5 –∑–∞–≤–µ—Ä—à–µ–Ω
    CV ROC-AUC: 0.893087
    –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    ROC-AUC –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: 0.893609
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
    

### 6.7. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è submission –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏

–î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è test —Ç–µ–º –∂–µ —Å–ø–æ—Å–æ–±–æ–º, —á—Ç–æ –∏ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –¥–ª—è Kaggle.


```python
print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è submission")

folds_dir = MODELS_DIR / 'catboost_folds'
model_path = MODELS_DIR / 'catboost_optuna_models.pkl'

if model_path.exists():
    try:
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {model_path}")
        saved_data = joblib.load(model_path)
        if 'models' in saved_data:
            models_optuna = saved_data['models']
            cat_features_optuna = saved_data['cat_features']
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(models_optuna)} —Ñ–æ–ª–¥–æ–≤ –∏–∑ .pkl")
        elif 'model_paths' in saved_data:
            models_optuna = []
            for model_path_str in saved_data['model_paths']:
                model = CatBoostClassifier()
                model.load_model(model_path_str)
                models_optuna.append(model)
            cat_features_optuna = saved_data['cat_features']
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(models_optuna)} —Ñ–æ–ª–¥–æ–≤ –∏–∑ .cbm —Ñ–∞–π–ª–æ–≤")
        else:
            raise ValueError("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ .pkl: {e}, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ .cbm —Ñ–∞–π–ª–æ–≤")
        trained_folds_path = folds_dir / 'trained_folds.npy'
        if trained_folds_path.exists():
            trained_folds = np.load(trained_folds_path).tolist()
            models_optuna = []
            for fold in trained_folds:
                model_path_cbm = folds_dir / f'catboost_fold_{fold}.cbm'
                if model_path_cbm.exists():
                    model = CatBoostClassifier()
                    model.load_model(str(model_path_cbm))
                    models_optuna.append(model)
            cat_features_optuna = None
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(models_optuna)} —Ñ–æ–ª–¥–æ–≤ –∏–∑ .cbm —Ñ–∞–π–ª–æ–≤")
        else:
            raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –æ–±—É—á–µ–Ω–Ω—ã–µ —Ñ–æ–ª–¥—ã –≤ {folds_dir}")
else:
    print(f"–§–∞–π–ª {model_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ .cbm —Ñ–∞–π–ª–æ–≤")
    trained_folds_path = folds_dir / 'trained_folds.npy'
    if trained_folds_path.exists():
        trained_folds = np.load(trained_folds_path).tolist()
        models_optuna = []
        for fold in trained_folds:
            model_path_cbm = folds_dir / f'catboost_fold_{fold}.cbm'
            if model_path_cbm.exists():
                model = CatBoostClassifier()
                model.load_model(str(model_path_cbm))
                models_optuna.append(model)
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(models_optuna)} —Ñ–æ–ª–¥–æ–≤ –∏–∑ .cbm —Ñ–∞–π–ª–æ–≤")
        cat_features_optuna = None
    else:
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –æ–±—É—á–µ–Ω–Ω—ã–µ —Ñ–æ–ª–¥—ã –≤ {folds_dir}")

if len(models_optuna) == 0:
    raise FileNotFoundError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏")

print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è submission")

test_data_full = pd.read_csv(DATA_DIR / 'test.csv')
print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(test_data_full):,}")

X_test = test_data_full.drop(['id'], axis=1, errors='ignore')

train_sample_for_preprocess = pd.read_csv(DATA_DIR / 'train.csv').head(10000)
X_train_sample = train_sample_for_preprocess.drop(['Response', 'id'], axis=1, errors='ignore')

X_test_proc, _ = simple_preprocess_catboost(X_test, X_train_sample)

if cat_features_optuna is None:
    cat_features_optuna = X_test_proc.columns.values

X_test_pool = Pool(X_test_proc, cat_features=cat_features_optuna)

print("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ test –¥–∞–Ω–Ω—ã—Ö")

batch_size = 100000
test_pred_proba = np.zeros(len(X_test_proc))

for i in range(0, len(X_test_proc), batch_size):
    end_idx = min(i + batch_size, len(X_test_proc))
    batch = X_test_proc.iloc[i:end_idx]
    batch_pool = Pool(batch, cat_features=cat_features_optuna)
    
    batch_preds = np.mean([m.predict_proba(batch_pool)[:, 1] for m in models_optuna], axis=0)
    test_pred_proba[i:end_idx] = batch_preds
    
    if (i // batch_size + 1) % 10 == 0:
        print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {end_idx:,} / {len(X_test_proc):,} —Å—Ç—Ä–æ–∫")

submission_custom = pd.DataFrame({
    'id': test_data_full['id'].values,
    'Response': test_pred_proba
})

submission_file_notebook = SUBMIT_DIR / 'custom_catboost_optuna_submission.csv'
submission_custom.to_csv(submission_file_notebook, index=False)

print(f"Submission —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {submission_file_notebook}")
print(f"–†–∞–∑–º–µ—Ä: {submission_custom.shape}")
print(f"–î–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: [{test_pred_proba.min():.6f}, {test_pred_proba.max():.6f}]")
print(f"–°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {test_pred_proba.mean():.6f}")

del models_optuna, X_test_pool, X_test_proc
import gc
gc.collect()
```

    –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è submission
    –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ src\models\catboost_optuna_models.pkl
    –ó–∞–≥—Ä—É–∂–µ–Ω–æ 5 —Ñ–æ–ª–¥–æ–≤ –∏–∑ .cbm —Ñ–∞–π–ª–æ–≤
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è submission
    –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: 7,669,866
    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ test –¥–∞–Ω–Ω—ã—Ö
      –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 1,000,000 / 7,669,866 —Å—Ç—Ä–æ–∫
      –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 2,000,000 / 7,669,866 —Å—Ç—Ä–æ–∫
      –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 3,000,000 / 7,669,866 —Å—Ç—Ä–æ–∫
      –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 4,000,000 / 7,669,866 —Å—Ç—Ä–æ–∫
      –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 5,000,000 / 7,669,866 —Å—Ç—Ä–æ–∫
      –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 6,000,000 / 7,669,866 —Å—Ç—Ä–æ–∫
      –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ 7,000,000 / 7,669,866 —Å—Ç—Ä–æ–∫
    Submission —Å–æ—Ö—Ä–∞–Ω–µ–Ω: src\submission\custom_catboost_optuna_submission.csv
    –†–∞–∑–º–µ—Ä: (7669866, 2)
    –î–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: [0.000004, 0.969301]
    –°—Ä–µ–¥–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: 0.122738
    




    54099



## 7. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Kaggle

–í—Å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±—ã–ª–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º—É Kaggle –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ. –ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö submission, –æ–Ω–∏ –Ω–µ–º–Ω–æ–≥–æ –ª—É—á—à–µ —á–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –Ω–æ—É—Ç–±—É–∫–µ, —Ç–∞–∫ –∫–∞–∫ –±—ã–ª–∏ –ø–æ–ø—ã—Ç–∫–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –±–æ–ª—å—à–∏—Ö –æ–±—å–µ–º–∞—Ö –¥–∞–Ω–Ω—ã—Ö

![kaggle](\src\other\kaggle.png)

- –ù–∞–∏–ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∫–∞–∑–∞–ª catboost —Å optuna `0.89518`
- LightGBM c optuna `0.88089`
- –ë–µ–π–∑–ª–∞–π–Ω –Ω–∞ LightAutoML `0.87945`

### 7.2. –ê–Ω–∞–ª–∏–∑ –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–∑–∏—Ü–∏—é –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –¥—Ä—É–≥–∏—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è


```python
leaderboard = pd.read_csv('../playground-series-s4e7-publicleaderboard-2025-12-16T20_03_46.csv')
leaderboard_sorted = leaderboard.sort_values('Score', ascending=False).reset_index(drop=True)
total = len(leaderboard)

results = {
    'OptunaCatboost': 0.89541,
    'Simple–°atboost': 0.88993,
    'AdvancedCatboost': 0.88929,
    'LightGBM': 0.88141,
    'LightAutoML': 0.87958
}

analysis = []
for name, score in sorted(results.items(), key=lambda x: x[1], reverse=True):
    rank = leaderboard_sorted[leaderboard_sorted['Score'] <= score].index[0] + 1
    top_pct = len(leaderboard_sorted[leaderboard_sorted['Score'] > score]) / total * 100
    analysis.append({
        'Submission': name, 
        'Public Score': score, 
        'Rank': rank, 
        'Top %': round(top_pct, 2)
    })

results_df = pd.DataFrame(analysis)

best_score = leaderboard['Score'].max()
worst_score = leaderboard['Score'].min()
mean_score = leaderboard['Score'].mean()
median_score = leaderboard['Score'].median()

print(f"–í—Å–µ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {total}\n")
print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–∞:")
print(f"–õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_score:.6f}")
print(f"–ú–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {median_score:.6f}")
print(f"–°—Ä–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {mean_score:.6f}")
print("\n–ü–æ–∑–∏—Ü–∏–∏ –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ:\n")
print(results_df.to_string(index=False))
```

    –í—Å–µ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤: 2236
    
    –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–∞:
    –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: 0.897930
    –ú–µ–¥–∏–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: 0.876265
    –°—Ä–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: 0.806191
    
    –ü–æ–∑–∏—Ü–∏–∏ –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ:
    
          Submission  Public Score  Rank  Top %
      OptunaCatboost       0.89541   339  15.12
      Simple–°atboost       0.88993   507  22.63
    AdvancedCatboost       0.88929   520  23.21
            LightGBM       0.88141   719  32.11
         LightAutoML       0.87958   925  41.32
    

## 8. –í—ã–≤–æ–¥—ã –∏ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ

### –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:

1. **EDA –∞–Ω–∞–ª–∏–∑** –ø–æ–∫–∞–∑–∞–ª –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –∏ –≤–∞–∂–Ω–æ—Å—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π

2. **LightAutoML baseline** –ø–æ–∫–∞–∑–∞–ª —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –¥–≤—É–º—è —Ä–∞–∑–Ω—ã–º–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º–∏. –≠—Ç–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —á—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã —Ä–∞–±–æ—Ç–∞—é—Ç —Ö–æ—Ä–æ—à–æ

3. **–°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ** —Å CatBoost –¥–∞–ª–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å baseline. –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ–∫–∞–∑–∞–ª—Å—è —Å–∞–º—ã–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º

4. **Feature engineering** –Ω–µ –¥–∞–ª –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏. –≠—Ç–æ –∑–Ω–∞—á–∏—Ç —á—Ç–æ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏

5. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤** —á–µ—Ä–µ–∑ Optuna –ø–æ–º–æ–≥–ª–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏

### –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã:

* CatBoost –ø–æ–∫–∞–∑–∞–ª –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Ç–æ–º—É —á—Ç–æ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
* –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–∫–∞–∑–∞–ª–∞—Å—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —á–µ–º —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
* –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–∞–∂–Ω–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
* –ü—Ä–æ—Å—Ç—ã–µ –ø–æ–¥—Ö–æ–¥—ã –º–æ–≥—É—Ç –±—ã—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —á–µ–º —Å–ª–æ–∂–Ω—ã–µ feature engineering —Ç–µ—Ö–Ω–∏–∫–∏

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:

–í–æ –≤—Ä–µ–º—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –±—ã–ª–∏ –ø—Ä–æ–≤–µ–¥–µ–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø–æ–∫–∞–∑–∞–Ω—ã –≤ —ç—Ç–æ–º –±–ª–æ–∫–Ω–æ—Ç–µ:

* –†–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∞–Ω—Å–∞–º–±–ª–µ–π –º–æ–¥–µ–ª–µ–π: stacking, blending
* –î—Ä—É–≥–∏–µ –º–µ—Ç–æ–¥—ã –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞–Ω–æ–º–∞–ª–∏–π
* –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –¥—Ä—É–≥–∏–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞

–≠—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª–∏ —á—Ç–æ –ø—Ä–æ—Å—Ç–æ–π CatBoost –ø–æ–¥—Ö–æ–¥ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —ç—Ç–æ –ª—É—á—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏.
